---
output:
  bookdown::pdf_document2:
    toc: yes
    number_sections: no
  bookdown::html_document2:
    toc: yes
    toc_float: yes
    fig_caption: yes
    code_folding: hide
    number_sections: no
  html_document:
    toc: yes
    df_print: paged
  pdf_document:
    toc: yes
bibliography: ../paper/library.bib
csl: apa6.csl
header-includes: \usepackage{caption} \renewcommand{\figurename}{Supplementary Figure} \renewcommand{\tablename}{Supplementary Table}
---

```{r, include = F}
knitr::opts_chunk$set(echo=F, warning=FALSE, message=FALSE, size="small")

opts <- options(knitr.kable.NA = "")
```

```{r, cache = F, include = F}
library(png)
library(grid)
library(xtable)
library(tidybayes)
library(ggridges)
library(glue)
library(brms)
library(stringr)
library(forcats)
library(ggthemes)
library(ggpubr)
library(reshape)
library(ggExtra)
library(tidybayes)
library(tidyboot)
library(MplusAutomation)
library(kableExtra)
library(knitr)
library(ggstance)
library(magick)
library(pdftools)
library(projpred)
library(bayesplot)
library(loo)
library(directlabels)
library(tidyverse)
library(cowplot)
library(readxl)

source("../utils/custom_facet.R")

cor_func <- function(x) {
  x %>%
    corrr::correlate()%>%
    gather(time_point, cor, -term)%>%
    mutate(cor = replace(cor, duplicated(cor), NA))%>%
    drop_na(cor)
}

library(coda)

estimate_mode <- function(s) {
  d <- density(s)
  return(d$x[which.max(d$y)])
}

hdi_upper<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","upper"])
}

hdi_lower<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","lower"])
}

```

```{r}
# read in data files
data_task <- read.csv("../data/laac2_data_task.csv")%>%
  mutate(task = recode(task,
       "population" = "Population-to-sample",
       "communication" = "Communicative-cues",
       "reasoning" = "Logical-reasoning",
       "visible_food" = "Ignore-visible-food",
       "inhibit_searched" = "Self-ordered-search",
       "attention" = "Attention-following"
  ))

data_trial <- read.csv("../data/laac2_data_trial.csv")%>%
  mutate(task = recode(task,
       "population" = "Population-to-sample",
       "communication" = "Communicative-cues",
       "reasoning" = "Logical-reasoning",
       "visible_food" = "Ignore-visible-food",
       "inhibit_searched" = "Self-ordered-search",
       "attention" = "Attention-following"
  ))
```



# Overview

This document supplements the paper "Individual differences in great ape cognition across time and domains: stability, structure, and predictability". Some text passages and figures are the same in the supplementary material and the main paper. This redundancy is intended and ensures that the supplementary material is self-contained and readable. 

This document is structured as follows: we first describe the [sample](#participants). Next we describe the general [setup](#setup) and the experimental [tasks](#tasks). In the section [data collection](#data-collection), we lay out the time line of data collection. Next, we give an overview of the [predictor variables](#predictors) we recorded in addition to the experimental data.

We then move on to our [analytical framework](#analytical-framework). First we describe the [Structural Equation models](#structural-equation-modeling) that were used to investigate robustness, stability and reliability of cognitive performance. Next, we describe the [Projection Predictive Inference](#projection-predictive-inference) analysis which we used to test the importance of the predictor variables.

In the [results](#results) we first report on the [robustness, stability and reliability](#stability-and-reliability) of each task. Next, we investigate [relations between](#relations-between-tasks) trait variables computed for the different tasks. Here we also include the tasks described in @bohn2023great. Finally, we report how the different [predictors](#predictability) related to performance in the different tasks.

# Methods 

## Participants

```{r}
participants <- data_trial%>%
  mutate(group = as.character(group), 
         species = ifelse(grepl("chimp",group),"chimpanzee", group), 
         species = factor(species))%>%
  group_by(species)%>%
  mutate(minage = round(min(age),1),
         maxage = round(max(age),1))%>%
  group_by(species, sex, minage,maxage)%>%
  summarise(n = length(unique(subject)))


 tpn <- data_trial%>%
     mutate(group = as.character(group), 
         species = ifelse(grepl("chimp",group),"chimpanzee", group), 
         species = factor(species))%>%
   group_by(time_point)%>%
   mutate(min_date = min(date))%>%
   group_by(time_point,group,.drop=FALSE)%>%
  summarise(n = length(unique(subject)),
            date = min(min_date))%>%
  group_by(time_point)%>%
  mutate(total_n = sum(n),
         date = as.Date(as.character(min(date)), "%Y%m%d"))%>%
   group_by(group)%>%
   mutate(end = date, 
          start = lag(as.character(end)))%>%
   mutate(start = ifelse(is.na(start),"2022-04-28",start),
          start = as.Date(start, format = "%Y-%m-%d"))%>%
  ungroup()%>%
  add_row(group = "total")

```

A total of `r sum(participants$n)` great apes participated at least in one tasks at one time point. This included `r participants%>%filter(species == "bonobos")%>%pull(n)%>%sum()` Bonobos (`r participants%>%filter(species == "bonobos", sex == "f")%>%pull(n)` females, age `r participants%>%filter(species == "bonobos", sex == "f")%>%pull(minage)` to `r participants%>%filter(species == "bonobos", sex == "f")%>%pull(maxage)`), `r participants%>%filter(species == "chimpanzee")%>%pull(n)%>%sum()` Chimpanzees (`r participants%>%filter(species == "chimpanzee", sex == "f")%>%pull(n)` females, age `r participants%>%filter(species == "chimpanzee", sex == "f")%>%pull(minage)` to `r participants%>%filter(species == "chimpanzee", sex == "f")%>%pull(maxage)`), `r participants%>%filter(species == "gorillas")%>%pull(n)%>%sum()` Gorillas (`r participants%>%filter(species == "gorillas", sex == "f")%>%pull(n)` females, age `r participants%>%filter(species == "gorillas", sex == "f")%>%pull(minage)` to `r participants%>%filter(species == "gorillas", sex == "f")%>%pull(maxage)`), and `r participants%>%filter(species == "gorillas")%>%pull(n)%>%sum()` Orangutans (`r participants%>%filter(species == "orangs", sex == "f")%>%pull(n)` females, age `r participants%>%filter(species == "orangs", sex == "f")%>%pull(minage)` to `r participants%>%filter(species == "orangs", sex == "f")%>%pull(maxage)`). The sample size at the different time points ranged from `r min(tpn$total_n, na.rm = T)` to `r max(tpn$total_n, na.rm = T)` for the different species. Figure \@ref(fig:sample) visualizes the sample size across time points. We tried to test all apes at all time points but this was not always possible due to a lack of motivation or logistical constraints (e.g. construction works). All apes participated in cognitive research on a regular basis. Many of them had experience with the tasks we used in the current study (see [predictor variables](#time-spent-in-research)).

Apes were housed at the Wolfgang KÃ¶hler Primate Research Center located in Zoo Leipzig, Germany. They lived in groups, with one group per species and two chimpanzee groups (group A and B). Research was noninvasive and strictly adhered to the legal requirements in Germany. Animal husbandry and research complied with the European Association of Zoos and Aquaria Minimum Standards for the Accommodation and Care of Animals in Zoos and Aquaria as well as the World Association of Zoos and Aquariums Ethical Guidelines for the Conduct of Research on Animals by Zoos and Aquariums. Participation was voluntary, all food was given in addition to the daily diet, and water was available ad libitum throughout the study. The study was approved by an internal ethics committee at the Max Planck Institute for Evolutionary Anthropology.

```{r sample, out.width="100%",fig.width=8, fig.height = 3, fig.cap = "Sample size by species across the different time points. Time point specific predictor variables were collected during the time between two time points to predict the next."}

cols <- c(ptol_pal()(5),"black")

ggplot(tpn, aes(x = date, y = n, col = group))+
  geom_point(alpha = .6)+
  geom_line(data = tpn, alpha = .6)+
  geom_point(aes(y = total_n), col = "black", alpha = .6)+
  geom_line(data = tpn,aes(y = total_n), col = "black", alpha = .6)+
  theme_minimal()+
  ylim(0,48)+
  labs(y = "Sample Size", x = "")+
  scale_color_manual(name = "Group", 
                   limits = c("a_chimp", "b_chimp", "bonobos", "gorillas", "orangs", "total"),
                   labels =c("Chimpanzee gr. A", "Chimpanzee gr. B", "Bonobo", "Gorilla", "Orangutan", "Total"),
                   values = cols)+
  scale_x_date(date_breaks = "2 month",
             date_labels = "%b")+
  guides(fill = F)+
  theme(panel.border = element_rect(colour = "black", fill=NA, size = .5))
```
## Setup {#setup}

Apes were tested in familiar sleeping or observation rooms by a single experimenter. Whenever possible, they were tested individually. The basic setup comprised a sliding table positioned in front of a mesh or a clear plexiglas panel with three holes in it. The experimenter sat on a small stool and used an occluder to cover the sliding table (see Supplementary Figure \@ref(fig:setup)).

```{r setup, include = T, fig.cap = "Setup used for the six tasks. A) population-to-sample, B) logical-reasoning, C) communicative-cues, D) ignore-visible-food, E) self-ordered-search and F) attention-following. Text at the bottom shows order of task presentation and trial numbers", out.width="100%"}
knitr::include_graphics("../visuals/setup.png") 
```


## Tasks {#tasks}

The tasks we selected are based on published procedures and are commonly used in the field of comparative psychology. The original publications often include control conditions to rule out alternative, cognitively less demanding ways to solve the tasks. We did not include such controls here and only ran the experimental conditions. For each task, we refer to the publication we used to model our procedure. We ask the reader to read these papers if they want to know more about control conditions and/or a detailed discussion of the nature of the underlying cognitive mechanisms.

Example videos for each task can be found in the associated online repository in [`videos/`](https://github.com/ccp-eva/laac2/tree/master/videos).

### Attention-following

The Attention-following task was loosely modeled after @kaminski2004body. The setup consisted of two identical cups placed on the sliding table and a large opaque screen that was longer than the width of the sliding table (Supplementary Figure \@ref(fig:setup)F). The experimenter placed both cups on the table and showed the ape that they were empty. Then, the experimenter baited both cups in view of the ape and placed the opaque screen in the center between the two cups, perpendicular to the mesh. Next, the experimenter moved to one side and looked at the cup in front of them. Then, the experimenter pushed the sliding table forward and the ape was allowed to choose one of the cups by pointing at it. If the ape chose the cup that the experimenter was looking at, they received the food item. If they choose the other cup, they did not. Because the experimenter could not see if the ape chose the other cup, the trial was terminated if the ape was at the mesh (in a position to make a choice) and did not choose the cup the experimenter was looking at within 3s. We coded whether the ape chose the side the experimenter was looking at (correct choice) or not. Apes received twelve trials. The side at which the experimenter looked was counterbalanced with same number of looks to each side and looks to the same side not more than two times in a row.

We assume that apes follow the experimenters focus of attention to determine whether or not their request could be seen and thus be successful.

### Communicative-cues

This task was modeled after @schmid2017great. Three identical cups were placed equidistantly on a sliding table directly in front of the ape (Supplementary Figure \@ref(fig:setup)C). In the beginning of a trial, the experimenter showed the ape that all cups are empty. After placing an occluder between the subject and the cups, the experimenter held up one food item and moved it behind the occluder, visiting all three cups but baiting only one. Next, the occluder was lifted and E looked at the ape (ostensive cue), called the name, and looked at one of the cups, while holding on to it with one hand and tapping it with the other (continuous looking, 3 times tapping). Finally, the experimenter pushed the sliding table forward for the ape to make a choice. If the ape chose the baited cup, they received the reward â if not, not. We coded as correct choice if the ape chose the indicated cup. Apes received twelve trials. The location of the indicated cup was counterbalanced, with each cup being the target equally often and the same target not more than two times in a row.

We assume that apes use the experimenter's communicative cues to determine where the food is hidden.

### Ignore-visible-food

The task was modeled after @volter2022inhibitory. The task involved two opaque cups with an additional, sealed but transparent, compartment attached to the front of each cup (facing the ape). For one cup, the compartment contained a preferred food item that was clearly visible, for the other cup, the compartment was empty (Supplementary Figure \@ref(fig:setup)D). In the beginning of the trial, the two cups were placed upside down on the sliding table so that the ape could see that the opaque compartments of both cups were empty. Next, the experimenter baited one of the cups in full view of the subject. In non-conflict trials, the baited cup was the cup with the food item in the transparent compartment. In conflict trials, the baited cup was the cup with the empty compartment. After baiting the experimenter pushed the sliding table forwards and the ape could chose by pointing. If the baited cup was chosen, the ape received the food. Apes received 14 trials, twelve conflict trials and two non-conflict trials (1st and 8th trial). Only conflict trials were analyzed. The location of the cup with the baited compartment was counterbalanced, with the cup not being in the same location more than twice in a row.

We assume that apes need to inhibit selecting the visible food item and instead use their short-term memory to remember where the food was hidden. 

### Logical-reasoning

The task was modeled after @hanus2014maths. Three identical cups were presented side-by-side on a sliding table, with the cup in the middle sometimes positioned closer to the left cup and sometimes closer to the right. (Supplementary Figure \@ref(fig:setup)B). Two half-open boxes served as occluders to block the apeâs view when shuffling the cups. Each trial started by showing the ape that all three cups (one on one side of the table, two on the other) were empty. After placing the occluders over both sides of the table, the experimenter put one piece of food on top of each occluder. Next, the experimenter hid each piece of food under the cup(s) behind the occluders. In case of the occluder with the two cups, the food was randomly placed under one of the two cups while both cups were visited and even shuffled. Finally, both occluders were lifted and the table pushed forwards, allowing the ape to choose one of the three cups, from which they then received the content. We coded whether the ape chose the certain cup (i.e. the cup from the side of the table with only one cup). Apes received 12 trials. The side with one cup was counterbalanced, with the same constellation appearing not more than two times in a row on the same side.

We assume that apes would infer that the cup from the tray with only one cup certainly contains food while the other cups contain food only in 50% of cases.  


### Population-to-sample

The task was modeled after @eckert2018intuitive. During the test, apes saw two transparent buckets filled with pellets and carrot pieces (the carrot pieces had roughly the same size and shape as the pellets). Each bucket contained 80 food items. The distribution of pellets to carrot pieces was 4:1 in bucket A, and 1:4 in bucket B. Pellets are preferred food items in comparison to carrots. The experimenter placed both buckets on a table, one left, one right (Supplementary Figure \@ref(fig:setup)A). In the beginning of a trial, the experimenter picked up the bucket on the right side, tilted it forward so the ape could see inside, placed it back on the table and turned it around 360Â°. The same procedure was repeated with the other bucket. Next, the experimenter looked at the ceiling, inserted each hand in the bucket in front of it and drew one item from the bucket without the ape seeing which type (E picked always of the majority type). The food items remained hidden in the experimenter's fists. Next, the experimenter extended the arms (in parallel) towards the ape who was then allowed to make a choice by pointing to one of the fists. The ape received the chosen sample. In half of the trials, the experimenter crossed arms when moving the fists towards the ape to ensure that the apes made a choice between samples and not just chose the side where the favorable population was still visible. In between trials, the buckets were refilled to restore the original distributions. Apes received twelve trials. We coded whether the ape chose the sample from the population with the higher number of high quality food items. The location of the buckets (left and right) was counterbalanced, with the buckets in the same location no more than two times in a row. The crossing of the hands was also counterbalanced with no more than two crossings in a row.

We assume that apes reasoned about the probability of the sample being a high quality item based on observing the ratio in the population.

### Self-ordered-search

The task was modeled after @volter2019chimpanzees [see also @petrides1995impairments; @diamond1997prefrontal]. Three identical cups were placed equidistantly on a sliding table directly in front of the ape (Supplementary Figure \@ref(fig:setup)E). The experimenter baited all three cups in full view of the ape. Next, the experimenter pushed the sliding table forwards for the ape to choose one of the cups by pointing. After the choice, the table was pulled back and the ape received the food. After a 3s pause, the table was pushed forward again for a second choice. This procedure was repeated for a third choice. If the ape chose a baited cup, they received the food, if not, not. We coded the number of times the ape chose and empty cup (i.e. chose a cup they already chose before). Please note that this outcome variable differed from the other tasks in two ways: first, possible values were 0, 1, and 2 (instead of just 0 and 1) and second, a lower score indicated better performance. Apes received twelve trials. No counterbalancing was needed.

We assume that apes use their working memory abilities to remember where they had already searched and which cups still contained food.

## Data collection {#data-collection}

One time point meant running all tasks with all participants. Within each time point, the tasks were organized in three sessions (see Supplementary Figure \@ref(fig:setup)). Session 1 included the population-to-sample and logical-reasoning tasks, session 2 the communicative-cues and ignore-visible-food tasks and session 3 the self-ordered-search and attention-following tasks.

The order of tasks was the same for all subjects. So was the positioning of food items within each task. The counterbalancing can be found in the coding sheets in the online repository in `documentation/`. This exact procedure was repeated at each time point so that the results would be comparable across participants and time points. The three sessions were usually spread out across three adjacent days. For the larger chimpanzee group, they were spread out across six days, but so that each individual completed the tasks within thre days (a.i. 50% of the group where tester in the first three days, the other half in the last three days).

Data collection started on April 28th, 2022, lasted until October 7th, 2023 and included 10 time points. The interval between two time points was planned to be eight weeks. However, it was not always possible to follow this schedule so that some intervals were longer or shorter. Supplementary Figure \@ref(fig:sample) visualizes the intervals between time points. All the data was collected by the same experimenter and where live-coded within the session. The experimenter was alone in the test room most of the time; on rare occasions there was a second person present to observe (e.g., for training purposes or internships).

## Predictors {#predictors}

In addition to the data from the cognitive tasks, we collected data for a range of predictor variables. The goal was to find variables that are systematically related to inter- and/or intra-individual variation in cognitive performance. That is, we were interested to see which variables allow us to predict cognitive performance. The second part of the [analysis](#projection-predictive-inference) section describes the method we used to determine the predictive value of each variable.

Predictors could either vary with the individual (stable individual characteristics; e.g. sex or rearing history), vary with individual and time point (variable individual characteristics; e.g. sickness or sociality), vary with group membership (group life; e.g. time spent outdoors or disturbances) or vary with the testing arrangements and thus with individual, time point and session (testing arrangements; e.g. presence of an observer or participation in other tests).

Most predictors were collected via a diary that the animal caretakers filled out on a daily basis. Here, the caretakers were asked a range of questions about the presence of a predictor and its severity.

### Stable individual characteristics

These predictors are stable individual differences. As a source, we used the ape handbook at Zoo Leipzig. Supplementary Figure \@ref(fig:demo) gives an overview of the distribution of the different characteristics in the sample.

#### Group

Group the individual belonged to. Groups were composed of individuals of the same species but because there were two chimpanzee groups (A-chimpanzees and B-chimpanzees), group and species are not equivalent. Variable name in model: `Group`.

#### Age

Absolute age of the individual. For some older individuals, only the year of birth was known. In these cases we calculated age with January 1st of that year as the birthday. Variable name in model: `Age`.

#### Sex

Participant's biological sex. Variable name in model: `Sex`.

#### Rearing history

Here, we differentiated between, `mother-reared`, `hand-reared` and `unknown`. The last category was used only for three chimpanzees. In the analysis, we classified them as `hand-reared` to facilitate model fitting (i.e. it is very difficult to estimate a parameter for a factor level with so little data). We think this decision is justified because the individuals in question have spent most of their life in close contact to humans and not in a larger chimpanzee group. Variable name in model: `Rearing history`.

#### Time spent in research

Absolute time the individual has lived in Leipzig Zoo. All apes living in Leipzig are involved in behavioral research to a certain degree. Thus, we take this measure to be a rough proxy of how much experience an individual has had with cognitive research. Variable name in model: `Time spent in research`.

```{r}
psex <- data_trial%>%
  filter(!is.na(sex))%>%
     mutate(group = as.character(group), 
         species = ifelse(grepl("chimp",group),"chimpanzee", group), 
         species = factor(species))%>%
  mutate(group = recode(group,
    a_chimp = "Chimpanzee gr. A",
    b_chimp = "Chimpanzee gr. B",
    bonobos = "Bonobo",
    orangs = "Orangutan",
    gorillas = "Gorilla"
  ))%>%
  group_by(group, sex)%>%
  summarise(Frequency = length(unique(subject)))%>%
  ggplot(aes(x = group, y = Frequency, fill = sex))+
  geom_histogram(stat = "identity", position = position_dodge(), col = "black")+
  theme_minimal(base_size = 8)+
  labs(x="", y = "Frequency")+
  scale_fill_grey(name = "Sex")+
  theme(legend.position = c(0.8,0.7), legend.key.size = unit(0.3, "cm"))+
  coord_flip()+
  theme(panel.border = element_rect(colour = "black", fill=NA, size = .5))

prearing <- data_trial%>%
  filter(!is.na(rearing))%>%
     mutate(group = as.character(group), 
         species = ifelse(grepl("chimp",group),"chimpanzee", group), 
         species = factor(species))%>%
    mutate(group = recode(group,
    a_chimp = "Chimpanzee gr. A",
    b_chimp = "Chimpanzee gr. B",
    bonobos = "Bonobo",
    orangs = "Orangutan",
    gorillas= "Gorilla"
  ))%>%
  group_by(group, rearing,.drop=FALSE)%>%
  summarise(Frequency = length(unique(subject)))%>%
  ggplot(aes(x = group, y = Frequency, fill = rearing))+
  geom_histogram(stat = "identity", position = position_dodge(preserve = "single"), col = "black")+
  theme_minimal(base_size = 8)+
  labs(x="", y = "Frequency")+
  scale_fill_viridis_d(name = "Rearing history")+
  theme(legend.position = c(0.8,0.7), legend.key.size = unit(0.3, "cm"))+
  coord_flip()+
  theme(panel.border = element_rect(colour = "black", fill=NA, size = .5))

page <- data_trial%>%
  filter(!is.na(age))%>%
     mutate(group = as.character(group), 
         species = ifelse(grepl("chimp",group),"chimpanzee", group), 
         species = factor(species))%>%
    mutate(group = recode(group,
    a_chimp = "Chimpanzee gr. A",
    b_chimp = "Chimpanzee gr. B",
    bonobos = "Bonobo",
    orangs = "Orangutan",
    gorillas = "Gorilla"
  ))%>%
  group_by(group, subject)%>%
  summarise(age = min(age))%>%
  ggplot(aes(x = age, y = group, fill = group))+
  geom_jitter(aes( col = group),alpha = .75, height = 0, width = .05, pch = "|", size = 5)+
  geom_density_ridges(col = "black", alpha = .5)+
  theme_minimal(base_size = 8)+
    labs(y="", x = "Subject age")+
  scale_fill_ptol(name = "Group")+
  scale_color_ptol(name = "Group")+
  guides(fill = F, col = F)+
  theme(panel.border = element_rect(colour = "black", fill=NA, size = .5))

pleipzig <- data_trial%>%
  filter(!is.na(time_in_leipzig))%>%
     mutate(group = as.character(group), 
         species = ifelse(grepl("chimp",group),"chimpanzee", group), 
         species = factor(species))%>%
    mutate(group = recode(group,
    a_chimp = "Chimpanzee gr. A",
    b_chimp = "Chimpanzee gr. B",
    bonobos = "Bonobo",
    orangs = "Orangutan",
    gorillas = "Gorilla"
  ))%>%
  group_by(group, subject)%>%
  summarise(time_in_leipzig = min(time_in_leipzig))%>%
  ggplot(aes(x = time_in_leipzig, y = group, fill = group))+
  geom_jitter(aes( col = group),alpha = .75, height = 0, width = .05, pch = "|", size = 5)+
  geom_density_ridges(col = "black", alpha = .5)+
  theme_minimal(base_size = 8)+
    labs(y="", x = "Time lived in Leipzig")+
  scale_fill_ptol(name = "Group")+
  scale_color_ptol(name = "Group")+
  guides(fill = F, col = F)+
  theme(panel.border = element_rect(colour = "black", fill=NA, size = .5))

```

```{r demo, out.width="100%", fig.height = 5, fig.width=8, fig.cap = "Stable individual characteristics. A) participant sex, B) age distribution by species, C) rearing history, D) time lived in leipzig by group."}

ggarrange(psex, page, prearing, pleipzig, nrow = 2, ncol= 2, widths = c(1,2), labels = c("A","B","C","D"))
```

### Variable individual characteristics

These predictors varied by participant and time point.

#### Rank

We asked caretakers to order individuals within a given group according to their rank. Ties were allowed. This was done at each time point. An individual's rank was mostly stable (see Supplementary Figure \@ref(fig:socrel)A) across time points, however, there was some variation. Variable name in model: `Rank`.

#### Sickness

As part of the caretakers' daily diary, we asked whether an individual was sick and if yes, how severe the sickness was on a scale from 1 to 7. For each time point, we used the mean of the daily sickness ratings as predictor. Variable name in model: `Sickness severity`.

#### Sociality

```{r}
#get date ranges for each time point
raw_data_date <- data_trial %>%
  mutate(subject = ifelse(subject == "shanga", "changa", subject))%>%
  filter(subject != "robert", subject != "dorien")%>%
  mutate(date = as.Date(as.character(date), format="%Y%m%d"))%>%
  select(date, time_point,group)%>%
  group_by(group,time_point)%>%
  summarise(end_date = min(date))%>%
  mutate(time_point = as.numeric(time_point))%>%
  dplyr::arrange(group,-time_point)%>%
  mutate(start_date = ifelse(time_point == 1, as.character(as.Date("2022-04-06", format="%Y-%m-%d")), as.character(end_date[-1])))%>%
  #mutate(dif = time_length(interval(start_date, end_date), "days"))
  mutate(end_date = as.Date(end_date-1, format = "%Y-%m-%d"),
         start_date = as.Date(start_date, format = "%Y-%m-%d"),
         time_point = factor(time_point))%>%
  group_by(time_point, group)%>%
  summarise(date = seq(start_date, end_date, by = "days"))%>%
  dplyr::rename(species = group)%>%
  mutate(species = paste(str_replace(species,"_","-"),"s",sep =""),
         species = str_replace(species,"ss","s"))

## get a list of all possible dyady for all possible time points
dyads <- list.files(path = "../data/social_relation_data",
                       pattern = "*.xlsx",
                       full.names = T)%>%
  map_df(~read_xlsx(.))%>%
  mutate(species = tolower(str_remove(`Configuration Name`, "LAAC ")),
         species = ifelse(species == "orangutans", "orangs", species),
         session = SessionID,
         date = as.Date(substr(as.character(DateTime),0,10),format="%Y-%m-%d"),
         focal = `Focal Name`,
         associates = `All Occurrence Behavior Social Modifier`,
         associates = str_remove(associates, "\\."),
         focal = str_remove(focal, "\\."))%>%
  #filter(date2 != "2020-08-21")%>%
  transform(associates = strsplit(associates,","))%>%
  unnest(associates)%>%
  select(species,focal,associates)%>%
  mutate(focal = ifelse(focal == "Shanga", "Changa", focal),
         associates = ifelse(associates == "Shanga", "Changa", associates))%>%
  gather(type, focal, -species)%>%
  filter(!is.na(focal))%>%
  distinct(species, focal)%>%
  mutate(associates = focal)%>%
  group_by(species)%>%
  tidyr::expand(focal,associates,c(1:n_distinct(data_trial$time_point)))%>%
  mutate(time_point = factor(`c(1:n_distinct(data_trial$time_point))`))%>%
  select(-`c(1:n_distinct(data_trial$time_point))`)%>%
  filter(focal != associates)%>%
  filter(focal != "Robert", focal != "Dorien",
         associates != "Robert", associates != "Dorien")

## read in the data from the observational scans
obs_data <- list.files(path = "../data/social_relation_data",
                       pattern = "*.xlsx",
                       full.names = T)%>%
  map_df(~read_xlsx(.))%>%
  mutate(species = tolower(str_remove(`Configuration Name`, "LAAC ")),
         species = ifelse(species == "orangutans", "orangs", species),
         session = SessionID,
         date = as.Date(substr(as.character(DateTime),0,10),format="%Y-%m-%d"),
         focal = `Focal Name`,
         associates = `All Occurrence Behavior Social Modifier`,
         associates = str_remove(associates, "\\."),
         focal = str_remove(focal, "\\."))%>%
  transform(associates = strsplit(associates,","))%>%
  unnest(associates)%>%
  select(species,session,date, focal,associates)%>%
  mutate(focal = ifelse(focal == "Shanga", "Changa", focal),
         associates = ifelse(associates == "Shanga", "Changa", associates))%>%
  filter(date > as.Date("2022-04-05",format="%Y-%m-%d"),
         date < as.Date("2023-11-10",format="%Y-%m-%d"))%>%
  left_join(raw_data_date)%>%
  filter(!is.na(time_point))%>%
  group_by(species,time_point)%>%
  mutate(n = length(unique(date)))%>% #compute the number of observations
  group_by(species,focal, associates,time_point)%>%
  summarise(count = n(), # count how often a combination of focal and associate occurred for a time point
            n = max(n))%>%# include the number of observations for that time point
  mutate(count = ifelse(is.na(associates),0,count))%>%
  rowwise() %>%
  filter(!is.na(associates))%>% # remove rows without associates
  ungroup()%>%
  filter(focal != "Robert", focal != "Dorien",
         associates != "Robert", associates != "Dorien")

## merge scan data with the dyads
raw_srm_data <- dyads %>%
  left_join(obs_data)%>%
  group_by(species, time_point)%>%
  mutate(n = max(n, na.rm = T))%>%
  mutate(count = ifelse(is.na(count),0,count))%>%
  rowwise() %>%
  mutate(dyad = paste(sort(c(focal, associates)), collapse = "_"))%>% # create ordered dyad column
  ungroup()

## prepare data for model run
srm_model_data <- raw_srm_data%>%
  group_by(time_point)%>%
  distinct(dyad, .keep_all = T)%>% # remove duplicate dyads to avoid counting same dyad twice
  ungroup()%>%
  filter(!count > n) # remove rows with more counts than observations

# sort(unique(srm_model_data$focal))
# sort(unique(srm_model_data$associates))
# 
# # ## run social relations model
# #
# # ### specify priors
# prior <-  c(prior(normal(0,2), class = Intercept),
#             prior(normal(0,2), class = b),
#             prior(normal(0,1), class = sd)
#             )
# 
# ### run model
# 
# srm_1 <-  brm(count | trials(n) ~ species + (0 + time_point | mm(focal, associates)) + ( 0 + time_point | dyad),
#            family = binomial(link = "logit"),
#            data = srm_model_data,
#            prior = prior,
#            control = list(adapt_delta = 0.95, 
#                           max_treedepth = 20),
#            threads = threading(8),
#            backend = "cmdstanr",
#            warmup = 2000,
#            iter   = 6000,
#            cores = 5, 
#            chains = 5)
# 
# 
# ### save output
# saveRDS(srm_1, "../saves/srm_1.rds")
# 
# ### load model results(too large to put on GitHub)
# srm_1 <-  readRDS("../saves/srm_1.rds")
# 
# 
# ### get species info for each subject
# subject_data <-  raw_srm_data%>%
#   ungroup()%>%
#   select(species, focal, associates)%>%
#   gather(role, subject,-species)%>%
#   distinct(subject, .keep_all = T)%>%
#   select(-role)%>%
#   arrange(species)%>%
#   mutate(subject = tolower(subject))%>%
#   mutate_at(.vars = vars(ends_with("s")),
#             .funs = funs(sub("s", "", .)))%>%
#   mutate(species = str_replace(species, "-", "_"))
# 
# ## extract model estimates for each subject
# subject_draws_1 <-  spread_draws(srm_1, r_mmfocalassociates[subject,time_point])%>% # extract posterior draws for each subject and time point
#   dplyr::rename(estimate = r_mmfocalassociates)%>%
#   mutate(subject = tolower(subject))%>%
#   left_join(subject_data) # merge with subject information to get species in
# 
# 
# ## summaries subject draws to put on GitHub
# subject_estimates <- subject_draws_1%>%
#   ungroup()%>%
#   mutate(time_point = as.numeric(str_remove(time_point, "time_point")))%>%
#   group_by(species, subject,time_point)%>%
#   summarise(mean = estimate_mode(estimate),
#             uci = hdi_upper(estimate),
#             lci = hdi_lower(estimate))
# 
# ## save subject draws to put on GitHub
# write_csv(subject_estimates, "../saves/srm_estimates.csv")

subject_estimates <- read_csv("../saves/srm_estimates.csv")

## generate plot
plot_est <-  subject_estimates%>%
  ungroup()%>%
  mutate(species_order = as.numeric(as.factor(species)),
         subject = factor(subject),
         subject = reorder(subject, species_order))


psrm <- plot_est%>%
  filter(species == "gorilla" 
         )%>%
mutate(subject = str_to_title(subject))%>%
ggplot(.,aes(y = mean, x = factor(time_point), col = subject)) +
  geom_point(alpha = .5)+
  #geom_vline(xintercept = 14.5)+
  #geom_pointrange(aes(y = mean, ymin = lci, ymax = uci), alpha = .5, position = position_dodge(width = .5))+
  geom_line(aes(group = subject),alpha = .5)+
  #facet_grid(~species)+
  theme_minimal()+
  scale_color_viridis_d(name ="Subject")+
  labs(x = "Time point", y = "Sociality estimate")+
  theme(panel.border = element_rect(colour = "black", fill=NA, size = .5))
```

We conducted proximity scans for all groups in the early afternoon on every workday (Monday to Friday)[*Nico: check if this is roughly correct*]. For each individual, we recorded which individuals were within arms reach. Research assistants used a tablet to record their observations with the behavioral coding software ZooMonitor [@wark2019monitoring]. Given the variable intervals between time points and constraints due to the availability of personnel, we did not have the same number of scans for each time point and species. On average, there were `r round(mean(srm_model_data$n),2)` scans per subject and time point (range: `r range(srm_model_data$n)[1]` - `r range(srm_model_data$n)[2]`).

To derive individual specific estimates of sociality for each time point, we fit a variant of a Social Relations Model [@snijders1999social] to the proximity data. These models allow estimating an individual specific sociality index while accounting for the dyadic nature of social interaction. Social relations models usually deal with directed behaviors (e.g. individual *i* is grooming individual *j*). Because the behavior we observed was symmetric, we cannot differentiate between the actor and receiver. \@Kajokaite2020 suggested to speak of a Multiple Membership Relations Model [see also @leckie2019multiple] in such a context, which simply estimates how likely an individual is to be observed in proximity to another individual.

In `brms` syntax, our model had the following structure: `count | trials(n) ~ group + (time_point | mm(focal, associates)) + (time_point | dyad)`. The dependent variable `count | trials(n)` is the number of times a dyad has been observed (`count`) at a time point relative to the number of scans taken for that time point (`trials(n)`). The fixed effect `group` estimates group difference in sociality. The random effect `(time_point | mm(focal, associates))` estimates the sociality for each individual. In that, the multi-membership grouping term `mm(focal, associates)` captures the fact that the assignment of the two roles (focal and associate) is arbitrary in the context of a symmetric behavior. The random slope `time_point` (treated as a factor) allowed us to estimate sociality for each time point. Finally, the random effect `(time_point | dyad)` accounts for dyad composition; in some cases a particular dyad composition (e.g. mother and infant) might be sufficient to explain high levels of sociality in an individual.

For each individual and time point, we extracted the sociality estimates and used them to predict cognitive performance in the different tasks for that time point. Supplementary Figure \@ref(fig:socrel)B visualizes the sociality measures for one group across the different time points. Variable name in model: `Sociality`.

```{r, out.width="100%", fig.height = 4, fig.cap = "Stable individual characteristics. A) participant sex, B) age distribution by species, C) rearing history, D) time lived in leipzig by species."}
prank <- data_trial%>%
     mutate(group = as.character(group), 
         species = ifelse(grepl("chimp",group),"chimpanzee", group), 
         species = factor(species))%>%
    mutate(group = recode(group,
    a_chimp = "Chimpanzee gr. A",
    b_chimp = "Chimpanzee gr. B",
    bonobos = "Bonobo",
    orangs = "Orangutan",
    gorillas = "Gorilla"
  ))%>%
  group_by(subject, group)%>%
  summarise(sd = sd(rank, na.rm = T))%>%
  ggplot(aes(x = sd, y = group, fill = group))+
  geom_jitter(aes(col = group), height = 0, width = .05, pch = "|", size = 5)+
  geom_density_ridges(alpha = .5, col = "black")+
  labs(x ="Variability in rank (sd)", y = "")+
  theme_minimal()+
  scale_fill_ptol()+
  scale_colour_ptol()+
  scale_x_continuous(limits = c(0, 3), oob = scales::squish)+
  guides(col = F, pch = F, fill =F)+
  theme(panel.border = element_rect(colour = "black", fill=NA, size = .5))
```

```{r socrel, out.width="100%", fig.height = 2.5, fig.width=8, fig.cap = "Variable individual characteristics. A) variability in rank (caretaker ratings) for each subject and species, B) sociality estimates for gorillas based on Multiple Membership Relations Model."}
ggarrange(prank,psrm, labels = c("A","B"), widths = c(1,1.5))
```

### Group life

These predictors varied by time point and group, but were the same for all individuals in that group. They were recorded in the animal caretaker diary. Supplementary Figure \@ref(fig:glife) visualizes the different variables across time points.

#### Time outdoors

Each day, the animal caretakers noted in the diary how many hours each group spent in the outdoor enclosure instead of the indoor enclosure or the sleeping rooms. To compute the predictor, we averaged across these values for each time point and group. Variable name in model: `Time spent outdoors`.

#### Disturbances

The animal caretakers also noted if there were any unusual disturbances for a particular group. Examples were construction works in the building, heavy weather conditions or green-keeping activities. In addition, the caretakers rated how disturbing they judged these events to be on a scale from 1 to 7. For each time point, we calculated the mean of these ratings. Variable name in model: `Disturbance`.

#### Life events

This variable captured whether there were any notable events within the group. Examples were fights in the group or the temporal removal of some individuals for medical procedures. Again, we asked the caretakers to rate the severity of these events on a scale from 1 to 7 and averaged across them. Variable name in model: `Life event`.

```{r}
plife <- data_trial%>%
  group_by(time_point,group)%>%
  mutate(group = recode(group,
    a_chimp = "Chimpanzee gr. A",
    b_chimp = "Chimpanzee gr. B",
    bonobos = "Bonobo",
    orangs = "Orangutan",
    gorillas = "Gorilla"
  ))%>%
  summarise(`Time spent outdoors` = mean(time_outdoors),
            `Disturbance` = mean(dist_mean),
            `Life event` = mean(le_mean))%>%
  pivot_longer(names_to = "measure", values_to = "value", cols = c(-group, -time_point))%>%
  ggplot(aes(x = factor(time_point), y = value, col = group))+
  geom_point()+
  geom_line(aes(group = group),alpha = .75)+
  facet_grid(measure~., scales = "free_y")+
  labs(x = "Time point", y = "")+
  scale_color_ptol(name = "Group")+
  theme_minimal()+
  theme(panel.border = element_rect(colour = "black", fill=NA, size = .5))

```

```{r glife, out.width="100%", fig.height = 4, fig.width=8, fig.cap = "Variation in group life related measures across groups and time points."}
plife
```

### Testing arrangements

```{r}
ptest <- data_trial%>%
  group_by(time_point,group)%>%
  mutate(group = recode(group,
    a_chimp = "Chimpanzee gr. A",
    b_chimp = "Chimpanzee gr. B",
    bonobos = "Bonobo",
    orangs = "Orangutan",
    gorillas = "Gorilla"
  ))%>%
  mutate(observer = ifelse(is.na(observer), 0, 1),
         test_day = ifelse(test_day == "no", 0, 1))%>%
  summarise(`Observer present` = mean(observer),
            `Study participation\n (day)` = mean(test_day),
            `Study participation\n (time point)` = mean(test_tp, na.rm = T))%>%
  pivot_longer(names_to = "measure", values_to = "value", cols = c(-group, -time_point))%>%
  ggplot(aes(x = factor(time_point), y = value, col = group))+
  geom_point()+
  geom_line(aes(group = group),alpha = .75)+
  facet_grid(measure~., scales = "free_y")+
  labs(x = "Time point", y = "")+
  scale_color_ptol(name = "Group")+
  theme_minimal()+
  theme(strip.text = element_text(size = 8), panel.border = element_rect(colour = "black", fill=NA, size = .5))
```

```{r gtest, out.width="100%", fig.height = 4, fig.width=8, fig.cap = "Variation in testing arrangenments across groups and time points. Top: Proportion of individuals that had an observer present while being tested. Middle: Proportion of individuals who participated in a different study on the same day. Bottom: Average number of studsies individuals participated in between time points."}
ptest
```

Testing arrangements varied between individuals, sessions and time points. The experimenter recorded them either based on their observations during testing or from the testing schedule, which lists all studies along with their participants that take place on a particular day. Supplementary Figure \@ref(fig:gtest) visualizes the different variables across time points.

#### Observer

We noted whether or not there was another animal in the same room or the room adjacent to the one the participant was in. Variable name in model: `Observer present`.

#### Study on same day

This predictor recorded whether or not the participant had already participated in a different study on the same day. The experimenter took this information from the testing schedule. Variable name in model: `Study participation (day)`.

#### Studies since last time point

Here we counted in how many other studies the participant had taken part in since the last time they were tested in that particular task. The experimenter took this information from the testing schedule. Variable name in model: `Study participation (time point)`.


# Analytical framework {#analytical-framework}

We had two overarching questions. On the one hand, we were interested in the cognitive measures and the relations between them. That is, we asked how robust performance in a given task was on a task-level, how stable individual differences were and how reliable the measures were. We also investigated relations between the different tasks. We used *Structural Equation Modeling* (SEM) [@bollen1989structural; @hoyle2012handbook] to address these questions. SEMs usually require larger sample sizes than available in the present study. In the Supplementary MAterial of @bohn2023great we reported a small simulation study which showed that parameters in the employed SEMs could be accurately estimated using Bayesian estimation techniques given our available sample sizes under reasonable model restrictions. We lay out the restrictive assumptions we imposed on the parameters in the text below.

Our second question was, which predictors explain variability in cognitive performance. Here we wanted to see which of the predictors we recorded were most important to predict performance over time. This is a variable selection problem (selecting a subset of variables from a larger pool) and we used *Projection Predictive Inference* for this [@piironen2018projective].

## Structural equation modeling {#structural-equation-modeling}

In the present analyses we were interested in estimating the stability of performances in a given task across time as well as the association between performances across different tasks. To separate components of random fluctuation (measurement error) from systematic differences in performance across time, we used Structural equation models (SEM). SEMs can be used to model relations between latent variables (constructs) which are estimated based on several observed variables.

*[Jana: following paragraph to be updated to include LST model with variable means]* We used latent state-trait models to separate traits (stable over time) from state residuals (time varying). In the present context, one can think of a trait as a stable psychological ability (e.g. ability to make causal inferences) and state residuals as time-specific deviations from these traits due to variable psychological conditions (e.g. variations in performance due to being attentive or inattentive). Variation in performance on a given time point can then be partitioned into variance due to the trait, variance due to the situation or individual-situation interactions, and measurement error. Because the latent variables are estimated on multiple indicators (here: test halves), they are assumed to be measurement-error free [@steyer1992states; @steyer2015theory; @geiser2020longitudinal]. Next we describe the model construction process in more detail.

At each time point, we observed 12 identical trials per individual per task. We modeled sum scores of the repeated trials (instead of latent ability factors), given that each trial per task was an identical repetition of the same task.

To separate reliable from unreliable variance components and obtain reliability estimates of the resulting sum score variables, we build two sum score variables per task per time point. That is, for each task, two parallel test halves were build, corresponding to performance sum scores of half of the trials of the same time point per task. Trials were alternately assigned to the first and the second test half. For all tasks except self-ordered-search this procedure resulted in two test halves with 7 possible values (0 to 6 correctly solved trials). For self-ordered-search, because one could search in the same location twice on each trial, test halves could maximally assume 13 possible values (0 to 12 errors). 

We interpreted reliability estimates in the following way: low < 0.5, moderate = 0.6, acceptable = .7, good = .8 and high = .9. Please note that these estimates are for test-halves and the reliability of the full test would be higher.

The two test halves served as indicators for a common latent construct per time point, assuming parallel test halves (i.e., factor loadings set to 1 and assuming equal reliability). Due to only few different observed values and skewed distributions of the sum score variables, indicators were modeled as ordered categorical variables, using a probit link function. The models thereby correspond to normal-ogive graded response models [@samejima1969estimation; @samejima1996graded]. That is, the models assume a continuous latent ability underlying the discrete responses, with an increasing probability of more correctly solved trials with increasing ability.

For model parsimony, to improve estimation accuracy, and in order to test for latent mean differences across time, we assume strict factorial (or measurement) invariance across time [@meredith1993measurement; @millsap2004assessing]. That is, in each model (task), loading parameters are set to 1 at all time points, residual variances are equal to 1 (by definition of the graded response model as detailed below), and threshold parameters (see below for details) are set invariant across time points. In other words, we assume that the indicators (test halves) measure the latent variable in an equivalent and stable manner over time.

### Models and coefficients

For each task, we constructed three different models which increased in complexity. We started with a latent state (LS) model, which estimates a latent state for each time point based on the two test halves. Robustness of task-level performance can be assessed by comparing latent state means across time points. Stability of individual differences can be assessed by correlating latent state variables across different time points.

Second, we fit a latent state-trait (LST) model. In LST models, true inter-individual differences are decomposed into a latent trait variable and time-specific deviations of the true score from the latent trait (state residual variable). In the following LST models, we assumed stable latent trait variables across time (no trait change). The model allowed us to partition the true variance in performance into stable (trait) and variable (state residual) components. Assuming a stable latent trait variable, the LST model is more restrictive than the LS model with respect to the implied covariance matrix, as correlations between true scores are not freely estimated across time points but assumed to be the same for different time lags.

*[Jana: quick explanation of this model]* Third, we fit LST models with varying means ... 

The following sections give a mathematical description of the different models and the parameters in them.

#### Latent state models

In the following we chose a factor analytical representation of the graded response model, that is, we present the models as factor models for ordinal data. Thereby we assume that the observed categorical variables $Y_{it}$ for test half $i$ at time point $t$ result from a categorization of unobserved continuous latent variables $Y^*_{it}$ which underlie the observed categorical variables. For observed variables that take on $k_{it}$ different ordered values out of the set of possible categories $S_{it} = 0, ..., k_{it}-1$ the relation between $Y_{it}$ and $Y^*_{it}$ is described by:

```{=tex}
\begin{equation}
Y_{it}=
\begin{cases} 
0~ & for ~~~Y_{it}^*\le \kappa_{1it} \\
s~~~~~~~~~ & for ~~~\kappa_{sit}<Y_{it}^*\le \kappa_{(s+1)it}~~~~~with~~0 < s < k_{it}-1\\
k_{it} - 1~~~ & for ~~~ \kappa_{(k_{it}-1)it}<Y_{it}^*
\end{cases}
\end{equation}
```
where $\kappa_{sit}$ denote threshold parameters [@muthen1984general].

The graded response model assumes that the different categories of responses (in our case the number of correct trials per test half) form an ordered scale. Which category an individual scores depends on their latent ability. Because the latent variable is continuous but the response is discrete, there are thresholds on the latent ability that mark the transition between response categories. The threshold parameters $\kappa_{sit}$ correspond to the level of the latent ability necessary to respond in category $s$ or higher with 0.50 probability.

In latent state models, these continuous latent variables $Y^*_{it}$ are decomposed into a latent state variable $S_t$ and a measurement error variable $\epsilon_{it}$ [see, for instance @eid2014statistical]:

```{=tex}
\begin{equation}
Y^*_{it}= S_t + \epsilon_{it}
\end{equation}
```
with $\epsilon_{it} \sim N(0,1)$ $\forall~ i,t$ (probit parameterization; normal-ogive graded response model). See @takane1987relationship for the equivalence of the normal-ogive graded response model and the factor model with ordinal indicators.

At each time point $t$, the two latent variables $Y^*_{1t}$ and $Y^*_{2t}$ are assumed to capture a common latent state variable $S_t$. Latent state variables are allowed to freely correlate across time, with latent (measurement-error free) correlations serving as indirect indicators of stability across time. The model is depicted for six measurement time points in Supplementary Figure \@ref(fig:lsgraph).

To test for possible mean changes of ability across time, the means of the latent state variables are freely estimated (assuming invariance of the threshold parameters $\kappa_{sit}$ across time).

As an estimate of reliability, the proportion of true score variance relative to the total variance of the continuous latent variables $Y^*_{it}$ is computed:

```{=tex}
\begin{equation}
Rel(Y^*_{it})=\frac{Var(S_t)}{Var(S_t)+Var(\epsilon_{it})}=\frac{Var(S_t)}{Var(S_t)+1}
\end{equation}
```

```{r lsgraph, engine='tikz', fig.cap="Latent State model for two indicators and six measurement time points.", out.width="25%", fig.align='center'}

\usetikzlibrary{arrows} 
\usetikzlibrary{positioning} 
\usetikzlibrary{shapes} 
\usetikzlibrary{fit} 
\usetikzlibrary{backgrounds} 
\usetikzlibrary{calc}

\begin{tikzpicture}
        [manifest/.style={rectangle,draw=black!80,semithick,minimum width=1.5cm,inner sep=
4pt,text centered},
        latent/.style={ellipse,draw=black!80,thick,inner sep=2,text centered},
        on/.style={->,>=stealth',semithick},
        from/.style={<-,>=stealth',semithick},
        with1/.style={<->,>=stealth',semithick,bend left=30},
        with2/.style={<->,>=stealth',semithick,bend right=30},
        with3/.style={<->,>=stealth',semithick,bend left=50},]

% Latent Variables %
 \begin{scope}[node distance=1.5]
                \node[latent] (o1) at (0,0) {$S_{1}$};
                \node[latent] (o2) [below=of o1] {$S_{2}$};
                \node[latent] (o3) [below=of o2] {$S_{3}$};
                \node[latent] (o4) [below=of o3] {$S_{4}$};
                \node[latent] (o5) [below=of o4] {$S_{5}$};
                \node[latent] (o6) [below=of o5] {$S_{6}$};
\end{scope}
                                                             	                        
% Manifest % 
 \foreach \a in {1,...,6} {\node (y1\a) [right = 2 of o\a] (yhelp\a) {};}        
 \foreach \b in {1,...,6} {\node[manifest] [above = .05 of yhelp\b] (y1\b) {$Y^*_{1\b}$}
 	edge [from] node[above] {1} (o\b);}
 \foreach \c in {1,...,6} {\node[manifest] [below = .05 of yhelp\c] (y2\c) {$Y^*_{2\c}$}
 	edge [from] node[below] {$1$} (o\c);} 

% Fehler %
\foreach \a in {1,...,5} {\node (e1\a) [below right = .3 of y1\a] {}
	edge [on] (y1\a.south east);}
\foreach \b in {1,...,5} {\node (e2\b) [below right = .3 of y2\b] {}
	edge [on] (y2\b.south east);}

\node (e16) [below right = .3 of y16] {$\epsilon_{16}$}
	edge [on] (y16.south east);
\node (e26) [below right = .3 of y26] {$\epsilon_{26}$}
	edge [on] (y26.south east);

%correlations%
\foreach \a in {1,...,5} {
\draw[<->,>=stealth',semithick] (o\a) to [with2]
node[align=center,below,yshift= 0mm, pos=0.5] 
                      {}  (o6);}
\foreach \a in {1,...,4} {
\draw[<->,>=stealth',semithick] (o\a) to [with2]
node[align=center,below,yshift= 0mm, pos=0.5] 
                      {}  (o5);}
\foreach \a in {1,...,3} {
\draw[<->,>=stealth',semithick] (o\a) to [with2]
node[align=center,below,yshift= 0mm, pos=0.5] 
                      {}  (o4);}
\foreach \a in {1,...,2} {
\draw[<->,>=stealth',semithick] (o\a) to [with2]
node[align=center,below,yshift= 0mm, pos=0.5] 
                      {}  (o3);}      
\draw[<->,>=stealth',semithick] (o1) to [with2]
node[align=center,below,yshift= 0mm, pos=0.5] 
                      {}  (o2);                

\end{tikzpicture}

```

#### Latent state-trait (LST) models

In LST models, the latent state variables $S_{it}$ are further decomposed into a latent trait variable $T_{it}$ and a latent state residual variable $\zeta_{it}$. The latent trait variables $T_{it}$ are time-specific dispositions, that is, trait scores capture the expected value of the latent state (i.e., true score) variable for an individual at time $t$ across all possible situations the individual might experience at time $t$ [@eid2017definition; @steyer2015theory]. The state residual variables $\zeta_{it}$ capture the deviation of a momentary state from the time-specific disposition $T_{it}$. In the following models, we assume that latent traits are stable across time. Additionally assuming common latent trait and state residual variables across the two test halves, results in the following measurement equation for parcel $i$ at time point $t$:

```{=tex}
\begin{equation}
Y^*_{it}= T + \zeta_t + \epsilon_{it}
\end{equation}
```
Here, $T$ is a stable (time-invariant) latent trait variable, capturing stable inter-individual differences between individuals. The state residual variable $\zeta_t$ captures time-specific deviations of the respective true score from the trait variable at time $t$, and thereby captures deviations from the trait due to situation or person-situation interaction effects. $\epsilon_{it}$ denotes a measurement error variable, with $\epsilon_{it} \sim N(0,1)$ $\forall~ i,t$. The model is depicted for 6 measurement time points in Supplementary Figure \@ref(fig:lstgraph).

```{r lstgraph, engine='tikz', fig.cap="Latent State-Trait model for two indicators and six measurement time points. All factor loadings of the latent trait factor $T$ are fixed to 1 (not displayed in the figure)", out.width="30%", fig.align='center'}

\usetikzlibrary{arrows} 
\usetikzlibrary{positioning} 
\usetikzlibrary{shapes} 
\usetikzlibrary{fit} 
\usetikzlibrary{backgrounds} 
\usetikzlibrary{calc}

\begin{tikzpicture}
        [manifest/.style={rectangle,draw=black!80,semithick,minimum width=1.5cm,inner sep=
4pt,text centered},
        latent/.style={ellipse,draw=black!80,thick,inner sep=2,text centered},
        on/.style={->,>=stealth',semithick},
        from/.style={<-,>=stealth',semithick},
        with1/.style={<->,>=stealth',semithick,bend left=30},
        with2/.style={<->,>=stealth',semithick,bend right=30},
        with3/.style={<->,>=stealth',semithick,bend left=50},]
        
% Latent Variables %
 \begin{scope}[node distance=1.5]
                \node[latent] (o1) at (0,0) {$\zeta_{1}$};
                \node[latent] (o2) [below=of o1] {$\zeta_{2}$};
                \node[latent] (o3) [below=of o2] {$\zeta_{3}$};
                \node[latent] (o4) [below=of o3] {$\zeta_{4}$};
                \node[latent] (o5) [below=of o4] {$\zeta_{5}$};
                \node[latent] (o6) [below=of o5] {$\zeta_{6}$};
\end{scope}
                     
% Trait % 
\node[latent,node distance=7] (t) [right=of  o4]  {$T_{}$};
                                             	                        
% Manifest % 
 \foreach \a in {1,...,6} {\node (y1\a) [right = 2 of o\a] (yhelp\a) {};}        
 \foreach \b in {1,...,6} {\node[manifest] [above = .05 of yhelp\b] (y1\b) {$Y^*_{1\b}$}
 	edge [from] node[above] {1} (o\b);}
 \foreach \c in {1,...,6} {\node[manifest] [below = .05 of yhelp\c] (y2\c) {$Y^*_{2\c}$}
 	edge [from] node[below] {$1$} (o\c);} 

% Fehler %
\foreach \a in {1,...,5} {\node (e1\a) [below right = .3 of y1\a] {}
	edge [on] (y1\a.south east);}
\foreach \b in {1,...,5} {\node (e2\b) [below right = .3 of y2\b] {}
	edge [on] (y2\b.south east);}

\node (e16) [below right = .3 of y16] {$\epsilon_{16}$}
	edge [on] (y16.south east);
\node (e26) [below right = .3 of y26] {$\epsilon_{26}$}
	edge [on] (y26.south east);

% Pfeile %
\foreach \a in {1,...,6}
	\path (y1\a.0) edge [from] (t);
\foreach \a in {1,...,6}	
	\path (y2\a.0) edge [from] (t);
\end{tikzpicture}

```

As noted above, we assume strict factorial (measurement) invariance. Additionally, for reasons of parsimony, we assume that the variances of the state residual variances are invariant across time. As a consequence, the specified LST model corresponds to a multilevel model with a latent trait factor at the between-level (person-level) and a latent state residual factor at the within-level (time-specific) level.

The following variance components can be computed for the presented LST model.

##### Consistency

Proportion of true variance (i.e., measurement-error free variance) that is due to true inter-individual stable trait differences.

```{=tex}
\begin{equation}
Con(Y^*_{it})=\frac{Var(T)}{Var(T)+Var(\zeta_t)}
\end{equation}
```
##### Occasion specificity

Proportion of true variance (i.e., measurement-error free variance) that is due to true inter-individual differences in the state residual variables (i.e., occasion-specific variation not explained by the trait).

```{=tex}
\begin{equation}
OS(Y^*_{it})=1-Con(Y^*_{it}) = \frac{Var(\zeta_t)}{Var(T)+Var(\zeta_t)}
\end{equation}
```
As state residual variances $Var(\zeta_t)$ were set equal across time, $Con(Y^*_{it})$ and $OS(Y^*_{it})$ are constant across time (as well as across item parcels $i$).

#### Latent state-trait (LST) models with varying means

*[Jana: flesh out this model]*


#### Correlations between tasks

*[Jana: Description of how the values for correlations between tasks were computed]*

### Estimation {#estimation}

*[Jana: quick check if this is still accurate]*

Models were estimated with MPlus version 8.4 [@muthen1998mplus], using Bayesian Markov-Chain Monte-Carlo sampling, with the Mplus default priors (see simulation studies in the [appendix](#appendix)). Using inverse gamma priors `IG(0.001, 0.001)` for LST models did not substantially change the parameter estimates (see simulation study). Therefore, only the results using the MPlus default priors are reported. We used two chains with a minimum of 10,000 iterations per chain, with a thinning of 10 (corresponds to a minimum of 100,000 drawn samples per chain of which every 10th is used for the construction of the posterior distribution). The first half of each chain is discarded as burn-in. Convergence was assumed and estimation stopped when the Potential Scale Reduction (PSR) factor was well below a threshold of 1.01 for the first time after the minimum number of iterations was reached. Model syntax can be accessed by locating the respective model in the folder `writing/supplement/saves/` in the GitHub repository and opening the `.out` file using a text editor.

Model fit was evaluated by computing Posterior Predicted P-values (PPP). PPP is the probability that the newly generated data are more extreme than the observed data, as measured by a specific test statistic or discrepancy function, in this case the chi-square fit function (that is, the likelihood ratio test between the specified structural equation model and an unrestricted mean and variance covariance model), see @asparouhov2010bayesian. The PPP is computed via the following steps: For a given MCMC iteration, a new data set is generated based on the model and the parameters of that iteration. Then the likelihood ratio chi-square test is applied to the real data as well as the newly generated data set to compute a fit index. The indices for the data and the generated data are then compared in size. If the value for the data is smaller, it is scored as 1 and if not, as 0. Averaging across these scores for the different iterations yields the PPP. Thus, values around .5 suggest a good model fit (no systematic difference between real and generated data) and very high and very low values suggest a poor model fit and / or model misspecification. In addition, we report the 95% CI of the difference between predicted and observed chi-square values, which should be centered around 0 for a good model fit.

In Mplus, every 10th iteration after burn-in is used to compute the PPP and the underlying continuous response variables $Y^*$ are used to compute the PPP in case of ordinal data.

## Projection predictive inference {#projection-predictive-inference}

The goal of this analysis was to select the predictor variables that are relevant for predicting performance in the different cognitive tasks over time. The selection of relevant predictor variables constitutes a variable selection problem, for which a range of different methods are available (e.g., shrinkage priors). We chose to use projection predictive inference because it is a state-of-the-art variable selection procedure that provides an excellent trade-off between model complexity and accuracy [@piironen2017comparison], especially when the goal is to identify a minimal subset of predictors that yield a good predictive model [@pavone2020using].

An overview of different projection techniques and an introduction to the projection prediction approach for generalized linear models can be found in @piironen2018projective. In this work, we use the extension to the generalized linear multilevel model case provided by @catalina2020projection.  
The projection prediction approach can be viewed as a two-step process: The first step consists of building the best predictive model possible, called the reference model. In the context of this work, the reference model is a Bayesian multilevel regression model (repeated measurements nested in apes), including all available predictors. The reference model serves as a performance goal regarding the predictive quality for the smaller models constructed by the projection prediction procedure. 

In the second step, the goal is to replace the posterior distribution of the reference model with a simpler distribution. This is achieved via a forward step-wise addition of predictors that decrease the Kullback-Leibler (KL) divergence from the reference model to the projected model. Let the reference model and the projected model have parameters $\theta'$ and $\theta$ respectively. Then by the definition of the KL divergence, the following optimization problem is obtained:
```{=tex}
\begin{equation}
\begin{aligned}
\theta_{\perp} &=\arg \min_\theta \frac{1}{n} \sum_{i=1}^{n} \mathrm{KL}\left(p\left(\tilde{y}_{i} \mid \theta'\right) \| p\left(\tilde{y}_{i} \mid \theta\right)\right)\\
&=\arg \min_\theta \frac{1}{n} \sum_{i=1}^{n} p\left(\tilde{y}_{i} \mid \theta'\right) \cdot \log \left(\frac{p\left(\tilde{y}_{i} \mid \theta'\right)}{p\left(\tilde{y}_{i} \mid \theta\right)}\right)
\end{aligned}
\end{equation}
```
The result of the projection is a list containing the best model for each number of predictors from which the final model is selected by inspecting the mean log-predictive density (`elpd`) and root-mean-squared error (`rmse`). The projected model with the smallest number of predictors which shows similar predictive performance as the reference model is chosen.

We built separate (Bayesian binomial multilevel regression) reference models for each task and ran them through the above-described projection prediction approach. The dependent variable for each task was the number of correctly solved trials in relation to the number of trials (i.e. the probability of solving a trial) per time point and task (`brms` notation: `p | trials(n) ~ predictors`, with `p` being the number of correct trials and `n` the number of trials). 

The self-ordered-search task had a structurally different dependent variable, namely a count variable for each trial (0,1 or 2 redundant searches). Modeling this data would have required a poisson model. Unfortunately, we were unable to implement such a model in the projection prediction approach. Thus, we transformed the dependent variable for the self-ordered-search task so that no redundant search was coded as "correct" and one or more redundant searches were coded as incorrect. This allowed us to analyse this task in the same way as all the others and makes the results comparable. However, we are aware that we loose information through this transformation.

Continuous predictors were centered when needed. We transformed the `rank` variable into a relative rank, where a rank of value one depicts a subject with the highest possible rank. All models also included `time_point` as a predictor to assess changes that are related to time and thus task experience (learning or habituation). All reference models converged well, having no divergent transitions, R-hat values equal to 1, and large ESSs for virtually all parameters. The R-hat value is a diagnostic value to investigate the convergence of the model and refers to the same concept as the potential scale reduction (PSR) factor defined above. R-hat values close to 1 indicate that the chains have mixed well (the estimates of the chains agree with each other), while values above 1 indicate that the chains did not converge to the same value. For chains of autocorrelated samples, the effective sample size (ESS) is an estimate for the number of independent samples within a chain containing the same amount of information about the dependent variable.

Following step two, we performed projection prediction for each reference model separately, thus resulting in different rankings of the relevant predictors for each task. We used the R package `projpred` [@projpred202], which implements the aforementioned projection prediction technique. The predictor relevance ranking is measured by the LOO cross-validated mean log-predictive density and root-mean-squared error. To find the optimal submodel size, we inspected -- in line with the authors' recommendations -- summaries and the plotted trajectories of the calculated `elpd` and `rmse`.

The order of relevance for the predictors and the random intercept (together called terms) is created by performing forward search. The term that decreases the KL divergence between the reference model's predictions and the projection's predictions the most goes into the ranking first. Forward search is then repeated $N$ times to get a more robust selection. We chose the final model by inspecting the predictive utility of each projection. To be precise, we chose the model with $p$ terms where $p$ depicts the number of terms at the cutoff between the term that increases the `elpd` and the term that does not increase the `elpd` by any significant amount. In order to get a useful predictor ranking, we manually delayed the random intercept term to the last position in the predictor selection process. The random intercept delay is needed because if the random intercept were not delayed, it would soak up almost all of the variance of the dependent variable before the predictors are allowed to explain some amount of the variance themselves. One could have used the function `suggest_size` as a heuristic decision rule to find the optimal submodel as an alternative to a graphical inspection. However, this is not yet possible due to the delay of the random intercept term.



# Results

```{r}
time_plot_0 <- data_task%>%
  #filter(time_point < 5)%>%
  drop_na(performance)%>%
  group_by(subject, task, time_point, group)%>%
  summarise(mean = mean(performance,na.rm = TRUE))

time_plot_1 <- data_task%>%
  #filter(time_point < 5)%>%
  drop_na(performance)%>%
  group_by(task, time_point, group)%>%
  summarise(mean = mean(performance,na.rm = TRUE))

time_plot_2 <- data_task%>%
  #filter(time_point < 5)%>%
  drop_na(performance)%>%
  #mutate(time_point = factor(time_point))%>%
  group_by(task, time_point)%>%
  tidyboot_mean(col = performance, na.rm = TRUE)%>%
  mutate(chance = ifelse(task == "Self-ordered-search", 1,ifelse(task == "Communicative-cues", 1/3,0.5)))

perfplot <- ggplot()+
  facet_wrap(~task, ncol = 3, scales = "free_y")+
  #geom_vline(data = time_plot_2,aes(xintercept = 14.5), lty = 1, col = "black", alpha = 1, size = 1)+
  geom_hline(data = time_plot_2,aes(yintercept = chance), lty = 2, col = "black", alpha = .75)+
   geom_jitter(data = time_plot_0, aes(x = factor(time_point), y = mean,  col = group), alpha = .1, width = 0.2)+
  geom_point(data = time_plot_1, aes(x = factor(time_point), y = mean,  col = group), alpha = .6)+
  geom_line(data = time_plot_1, aes(x = factor(time_point), y = mean, col = group, group = group), alpha = .6)+
  geom_pointrange(data = time_plot_2, aes(x = factor(time_point), y = mean, ymin = ci_lower, ymax = ci_upper),pch = 4)+
  geom_line(data = time_plot_2, aes(x = factor(time_point), y = mean, group = task))+
  theme_minimal()+
  labs(x = "Time point", y = "Performance")+
  guides(size = F)+
    scale_color_manual(name = "Group", 
                   limits = c("a_chimp", "b_chimp", "bonobos", "gorillas", "orangs"),
                   labels =c("Chimpanzee gr. A", "Chimpanzee gr. B", "Bonobo", "Gorilla", "Orangutan"),
                   values = cols)+
  #scale_x_continuous(breaks = c(1:max(time_plot_2$time_point)))+
  theme(legend.position = "right", panel.border = element_rect(colour = "black", fill=NA, size = .5))

```
```{r perfplot, fig.width = 10, fig.height = 6,fig.cap = "Results from the six cognitive tasks across time points. Black crosses show mean performance at each time point across species (with 95\\% CI). The sample size varied between time points and can be found in Supplementary Figure 1. Colored dots show mean performance by species. Dashed line shows the chance level whenever applicable.", out.width="100%"}

perfplot
```

```{r}
#ggsave("../visuals/perf.png", height = 5, width = 12, scale = 1, bg = "white")
```

```{r}
cor_data <- data_task%>%
  select(subject, task, time_point, performance)%>%
  pivot_wider(values_from = "performance", names_from = "time_point")%>%
  ungroup()%>%
  select(-subject)%>%
  group_by(task)%>%
  group_split(.keep = F)%>%
  setNames(data_task%>%filter(time_point < 15)%>%arrange(task)%>%distinct(task)%>%pull(task))%>%
  purrr::map(cor_func)%>%
  melt()%>%
  mutate(task = factor(L1))


rel_plot <- cor_data%>%
  mutate(time_span = factor(abs(as.numeric(term) - as.numeric(time_point))*6))%>%
  ggplot(aes(x = time_span, y = value))+
  geom_point(alpha = .5, size = 0.75)+
  geom_hline(yintercept = 0, lty = 2, col = "black", alpha = .75)+
  #geom_smooth(method = "lm", se = F, col = "black", lty = 2, size = .5)+
  #stat_cor(method = "pearson", aes(x = time_span, y = value, label = paste(..r.label..)), inherit.aes = F,r.accuracy = 0.01, cor.coef.name = "r", label.x = 1, label.y = 0.99,)+
  ylim(-0.2,1)+
  labs(y = "Correlation coefficient",
     x = "Weeks between time points") +
  facet_grid(~task)+
  theme_minimal()+
    theme(panel.border = element_rect(colour = "black", fill=NA, size = .5))
```



```{r relplot, out.width="100%", fig.width = 10, fig.height=3, set.cap.width=T, fig.cap = "Re-test correlation coefficients plotted against the temporal distance between the testing time points."}
rel_plot
```


## Robustness, Stability and Reliability {#stability-and-reliability}

To get an overview of the results, we first visualized the data. Supplementary Figure \@ref(fig:perfplot) shows performance at the different time points. From a task-level perspective, we can say that performance was consistently above chance in the communicative-cues, ignore-visible-food and population-to-sample tasks. For attention-following, this was the case only from time point 7 onward and for logical-reasoning, performance was, if anything, below chance. For the self-ordered-search task, performance was below chance but here lower values reflect better performance (i.e. systematic avoidance of the visible food item). For attention-following, ignore-visible-food, communicative-cues and self-ordered-search there was a steady improvement in performance over time.  

For a first glimpse on the stability of individual differences, we correlated performance at the different time points for each task (all possible combinations of time points). Supplementary Figure \@ref(fig:relplot) visualizes these re-test correlation coefficients against the temporal distance between time points. Correlations between time points were exclusively larger than zero for direct communicative-cues, population-to-sample and self-ordered-search. For quantity discrimination, this distribution was wider and overlapped with zero, but was still mostly positive. For attention-following, ignore-visible-food and logical-reasoning some correlations were negative. For all tasks, correlations between time points were lower for time points that were further apart [see also @uher2011individual]. However, these re-test correlations confound measurement precision (reliability) and stability of individual differences. That is, low correlations could reflect high measurement error or a lack of stability of individual differences. We tease these components apart in the SEM models reported below. 

Next, we report the SEM results for the different tasks and the relations between them. All models showed acceptable fit indices (see Supplementary Table \@ref(tab:semt)). The threshold parameters for each model are shown in Supplementary Table \@ref(tab:thresht).


```{r, include=F}
# latent state models
lsa <- readModels("../outputs/laac2_LS_attent.out")
lsc <- readModels("../outputs/laac2_LS_comm.out")
lsp <- readModels("../outputs/laac2_LS_population.out")
lsr <- readModels("../outputs/laac2_LS_reason.out")
lsi <- readModels("../outputs/laac2_LS_search.out")
lsv <- readModels("../outputs/laac2_LS_food.out")

# latent state trait models
lsta <- readModels("../outputs/laac2_LST_attent.out")
lstc <- readModels("../outputs/laac2_LST_comm.out")
lstp <- readModels("../outputs/laac2_LST_population.out")
lstr <- readModels("../outputs/laac2_LST_reason.out")
lsti <- readModels("../outputs/laac2_LST_search_2.out")
lstv <- readModels("../outputs/laac2_LST_food.out")

# latent state trait models fixed variances varying means

lstaf <- readModels("../outputs/laac2_LST_attent_equalSvar.out")
lstcf <- readModels("../outputs/laac2_LST_comm_equal_Svar.out")
lstpf <- readModels("../outputs/laac2_LST_population_equalSvar.out")
lstrf <- readModels("../outputs/laac2_LST_reason_equalSvar.out")
lstif <- readModels("../outputs/laac2_LST_search_2_equalSvar.out")
lstvf <- readModels("../outputs/laac2_LST_food_equalSvar.out")

# latent state trait models equal variances varying means

lstav <- readModels("../outputs/laac2_LST_attent_equalSvar_means.out")
lstav_short <- readModels("../outputs/laac2_LST_attent_equalSvar_means_short.out")
lstcv <- readModels("../outputs/laac2_LST_comm_equal_Svar_means.out")
lstpv <- readModels("../outputs/laac2_LST_population_equalSvar_means.out")
lstrv <- readModels("../outputs/laac2_LST_reason_equalSvar_means.out")
lstiv <- readModels("../outputs/laac2_LST_search_2_equalSvar_means.out")
lstvv <- readModels("../outputs/laac2_LST_food_equalSvar_means.out")

```

```{r}
mot <- bind_rows(
  tibble(
  Task = c("Attention-following", "", ""),
  Model = c("LSM","LSTM", "LSTM-V"),
  PPP = c(lsa$summaries$PostPred_PValue,
          lsta$summaries$PostPred_PValue,
          lstaf$summaries$PostPred_PValue),
  `Chi 95% CI` = c(
    
    paste(
      lsa$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2),
      lsa$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
    
    paste(
      lsta$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2),
      lsta$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
    
    paste(
      lstaf$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2),
      lstaf$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; ")
    
    )),
  
  
  tibble(
  Task = c("Communicative-cues", "", ""),
  Model = c("LSM","LSTM", "LSTM-V"),
  PPP = c(lsc$summaries$PostPred_PValue,
          lstc$summaries$PostPred_PValue,
          lstcf$summaries$PostPred_PValue),
  `Chi 95% CI` = c(
    
    paste(
      lsc$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2),
      lsc$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
    
    paste(
      lstc$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2),
      lstc$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
    
    paste(
      lstcf$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2),
      lstcf$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; ")
    
    )),
  
  
  tibble(
  Task = c("Population-to-sample", "", ""),
  Model = c("LSM","LSTM", "LSTM-V"),
  PPP = c(lsp$summaries$PostPred_PValue,
          lstp$summaries$PostPred_PValue,
          lstpf$summaries$PostPred_PValue),
  `Chi 95% CI` = c(
    
    paste(
      lsp$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2),
      lsp$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
    
    paste(
      lstp$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2),
      lstp$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
    
    paste(
      lstpf$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2),
      lstpf$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; ")
    
    )),
  
  
  tibble(
  Task = c("Logical-reasoning", "", ""),
  Model = c("LSM","LSTM", "LSTM-V"),
  PPP = c(lsr$summaries$PostPred_PValue,
          lstr$summaries$PostPred_PValue,
          lstrf$summaries$PostPred_PValue),
  `Chi 95% CI` = c(
    
    paste(
      lsr$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2),
      lsr$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
    
    paste(
      lstr$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2),
      lstr$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
    
    paste(
      lstrf$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2),
      lstrf$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; ")
    
    )),
  
  
  tibble(
  Task = c("Self-ordered-search", "", ""),
  Model = c("LSM","LSTM", "LSTM-V"),
  PPP = c(lsi$summaries$PostPred_PValue,
          lsti$summaries$PostPred_PValue,
          lstif$summaries$PostPred_PValue),
  `Chi 95% CI` = c(
    
    paste(
      lsi$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2),
      lsi$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
    
    paste(
      lsti$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2),
      lsti$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
    
    paste(
      lstif$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2),
      lstif$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; ")
    
    )),
  
  
  tibble(
  Task = c("Ignore-visible-food", "", ""),
  Model = c("LSM","LSTM", "LSTM-V"),
  PPP = c(lsv$summaries$PostPred_PValue,
          lstv$summaries$PostPred_PValue,
          lstvf$summaries$PostPred_PValue),
  `Chi 95% CI` = c(
    
    paste(
      lsv$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2),
      lsv$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
    
    paste(
      lstv$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2),
      lstv$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
    
    paste(
      lstvf$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2),
      lstvf$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; ")
    
    ))
)
```

```{r semt}
kable(mot, align = c("l", "l", "c", "c"), caption = "Model fit indices", booktabs=TRUE)%>%
kable_classic(full_width = F)%>%
      footnote(c("LSM = Latent state model", "LSTM = Latent state-trait model","LSTM-V = Latent state-trait model with varying means", "PPP = Posterior predictive p-value. Cut-off values (two-tailed): 0.025 and 0.975", "Chi 95% CI = 95%CI of difference between predicted and observed chi-square values"))
```


```{r}
thresh <- bind_rows(
  bind_rows(
    lsa$parameters$unstandardized%>%mutate(Model = "LSM", Task = "Attention-following"),
    lsta$parameters$unstandardized%>%mutate(Model = "LSTM", Task = ""),
    lstaf$parameters$unstandardized%>%mutate(Model = "LSTM-V", Task = ""),
    )%>%
    filter(paramHeader=="Thresholds")%>%
    select(Task, Model, param, est)%>%
    filter(grepl("P11\\$",param))%>%
    separate(param,into = c("tp","Threshold"), sep = "\\$")%>%
    mutate(Threshold = paste("T", Threshold, sep = ""))%>%
    select(-tp)%>%
    pivot_wider(names_from = Threshold, values_from = est),
  
   bind_rows(
    lsc$parameters$unstandardized%>%mutate(Model = "LSM", Task = "Communicative-cues"),
    lstc$parameters$unstandardized%>%mutate(Model = "LSTM", Task = ""),
    lstcf$parameters$unstandardized%>%mutate(Model = "LSTM-V", Task = ""),
    )%>%
    filter(paramHeader=="Thresholds")%>%
    select(Task, Model, param, est)%>%
    filter(grepl("P11\\$",param))%>%
    separate(param,into = c("tp","Threshold"), sep = "\\$")%>%
    mutate(Threshold = paste("T", Threshold, sep = ""))%>%
    select(-tp)%>%
    pivot_wider(names_from = Threshold, values_from = est),
  
  bind_rows(
    lsp$parameters$unstandardized%>%mutate(Model = "LSM", Task = "Population-to-sample"),
    lstp$parameters$unstandardized%>%mutate(Model = "LSTM", Task = ""),
    lstpf$parameters$unstandardized%>%mutate(Model = "LSTM-V", Task = ""),
    )%>%
    filter(paramHeader=="Thresholds")%>%
    select(Task, Model, param, est)%>%
    filter(grepl("P11\\$",param))%>%
    separate(param,into = c("tp","Threshold"), sep = "\\$")%>%
    mutate(Threshold = paste("T", Threshold, sep = ""))%>%
    select(-tp)%>%
    pivot_wider(names_from = Threshold, values_from = est),
  
  bind_rows(
    lsr$parameters$unstandardized%>%mutate(Model = "LSM", Task = "Logical-reasoning"),
    lstr$parameters$unstandardized%>%mutate(Model = "LSTM", Task = ""),
    lstrf$parameters$unstandardized%>%mutate(Model = "LSTM-V", Task = ""),
    )%>%
    filter(paramHeader=="Thresholds")%>%
    select(Task, Model, param, est)%>%
    filter(grepl("P11\\$",param))%>%
    separate(param,into = c("tp","Threshold"), sep = "\\$")%>%
    mutate(Threshold = paste("T", Threshold, sep = ""))%>%
    select(-tp)%>%
    pivot_wider(names_from = Threshold, values_from = est),
  
  bind_rows(
    lsi$parameters$unstandardized%>%mutate(Model = "LSM", Task = "Self-ordered-search"),
    lsti$parameters$unstandardized%>%mutate(Model = "LSTM", Task = ""),
    lstif$parameters$unstandardized%>%mutate(Model = "LSTM-V", Task = ""),
    )%>%
    filter(paramHeader=="Thresholds")%>%
    select(Task, Model, param, est)%>%
    filter(grepl("P11\\$",param))%>%
    separate(param,into = c("tp","Threshold"), sep = "\\$")%>%
    mutate(Threshold = paste("T", Threshold, sep = ""))%>%
    select(-tp)%>%
    pivot_wider(names_from = Threshold, values_from = est),
  
  bind_rows(
    lsv$parameters$unstandardized%>%mutate(Model = "LSM", Task = "Ignore-visible-food"),
    lstv$parameters$unstandardized%>%mutate(Model = "LSTM", Task = ""),
    lstvf$parameters$unstandardized%>%mutate(Model = "LSTM-V", Task = ""),
    )%>%
    filter(paramHeader=="Thresholds")%>%
    select(Task, Model, param, est)%>%
    filter(grepl("P11\\$",param))%>%
    separate(param,into = c("tp","Threshold"), sep = "\\$")%>%
    mutate(Threshold = paste("T", Threshold, sep = ""))%>%
    select(-tp)%>%
    pivot_wider(names_from = Threshold, values_from = est),
  

)%>%mutate_all(~replace(., is.na(.), ""))


```

```{r thresht}
kable(thresh, caption = "Threshold parameters", align = c("l", "l", "c", "c", "c", "c", "c"), booktabs=TRUE)%>%
kable_classic(full_width = F)%>%
      footnote(c("LSM = Latent state model", "LSTM = Latent state-trait model","LSTM-V = Latent state-trait model with varying means", "T1-5 = Threshold parameters for response categories"))
```


```{r}

plse <- bind_rows(
  lsa$parameters$unstandardized%>%mutate(task = "attention"),
  lsc$parameters$unstandardized%>%mutate(task = "communication"),
  lsp$parameters$unstandardized%>%mutate(task = "pop-to-sample"),
  lsr$parameters$unstandardized%>%mutate(task = "reasoning"),
  lsi$parameters$unstandardized%>%mutate(task = "inhibit-searched"),
  lsv$parameters$unstandardized%>%mutate(task = "visible-food"),
          # lsi$parameters$unstandardized%>%mutate(task = "inference"),
          # lsq$parameters$unstandardized%>%mutate(task = "quantity"),
          # lsg$parameters$unstandardized%>%mutate(task = "gaze_following")
)%>%
filter(paramHeader == "Means")%>%
  mutate(time_point = as.numeric(gsub("[^0-9.-]", "", param)),
         estimate = "Latent state mean",
         ref = 0)%>%
  mutate(task = recode(task,
       "pop-to-sample" = "Population-to-sample",
       "communication" = "Communicative-cues",
       "reasoning" = "Logical-reasoning",
       "visible-food" = "Ignore-visible-food",
       "inhibit-searched" = "Self-ordered-search",
       "attention" = "Attention-following"
  ))


plsr <- bind_rows(
  lsa$parameters$r2%>%mutate(task = "attention"),
  lsc$parameters$r2%>%mutate(task = "communication"),
  lsp$parameters$r2%>%mutate(task = "pop-to-sample"),
  lsr$parameters$r2%>%mutate(task = "reasoning"),
  lsi$parameters$r2%>%mutate(task = "inhibit-searched"),
  lsv$parameters$r2%>%mutate(task = "visible-food"),
          # lsi$parameters$unstandardized%>%mutate(task = "inference"),
          # lsq$parameters$unstandardized%>%mutate(task = "quantity"),
          # lsg$parameters$unstandardized%>%mutate(task = "gaze_following")
)%>%
  slice(which(row_number() %% 2 == 1))%>%
  mutate(time_point = as.numeric(str_remove(param, "P1")),
         estimate = "Reliability",
         ref = NA)%>%
  mutate(task = recode(task,
       "pop-to-sample" = "Population-to-sample",
       "communication" = "Communicative-cues",
       "reasoning" = "Logical-reasoning",
       "visible-food" = "Ignore-visible-food",
       "inhibit-searched" = "Self-ordered-search",
       "attention" = "Attention-following"
  ))
```

```{r}
lsm <- ggplot(plse, aes(x = time_point, y = est))+
  geom_hline(aes(yintercept = ref), lty = 3, alpha = .75)+
  labs(x = "Time point",y = "Latent state mean")+
  geom_line(alpha = .75)+
  #geom_line(data = lsep%>%filter(time_point > 14), aes(group = task, x = time_point),position = position_dodge(width = 0.2), alpha = .75)+
  geom_pointrange(aes(ymin = lower_2.5ci, ymax = upper_2.5ci, pch = sig), alpha = .75, fatten = 3)+
  facet_grid(~task)+
  scale_shape_manual(values = c(16,4), name = "", labels = c("includes 0", "different from 0"))+
  #geom_vline(xintercept = 14.5, size = 1)+
  scale_x_continuous(breaks = c(1,10), labels = c(1,10))+
  theme_few()+
  theme(legend.direction = "vertical", legend.position = c(0.075, 0.9), legend.box = "horizontal", legend.background = element_blank(), strip.background = element_blank(),
  strip.text.y = element_blank(), axis.title.x = element_blank(), axis.ticks.x = element_blank(), axis.text.x = element_blank(),
    axis.text = element_text(size = 8),       # Smaller axis text
    axis.title = element_text(size = 10),     # Smaller axis titles
    legend.text = element_text(size = 6),     # Smaller legend text
    strip.text = element_text(size = 8)      # Smaller plot title
    )

lsm2 <- ggplot(plse, aes(x = time_point, y = est))+
  geom_hline(aes(yintercept = ref), lty = 3, alpha = .75)+
  labs(x = "Time point",y = "Latent state mean")+
  geom_line(alpha = .75)+
  #geom_line(data = lsep%>%filter(time_point > 14), aes(group = task, x = time_point),position = position_dodge(width = 0.2), alpha = .75)+
  geom_pointrange(aes(ymin = lower_2.5ci, ymax = upper_2.5ci, pch = sig), alpha = .75, fatten = 3)+
  facet_grid(~task)+
  scale_shape_manual(values = c(16,4), name = "", labels = c("includes 0", "different from 0"))+
  #geom_vline(xintercept = 14.5, size = 1)+
  scale_x_continuous(breaks = c(1,10), labels = c(1,10))+
  theme_few()+
  theme(legend.direction = "vertical", legend.position = c(0.075, 0.9), legend.box = "horizontal", legend.background = element_blank(), strip.background = element_blank(),
  strip.text.y = element_blank(), axis.title.x = element_blank(),# axis.ticks.x = element_blank(), axis.text.x = element_blank(),
    #axis.text = element_text(size = 8),       # Smaller axis text
    #axis.title = element_text(size = 10),     # Smaller axis titles
    legend.text = element_text(size = 8),     # Smaller legend text
    #strip.text = element_text(size = 8)      # Smaller plot title
    )
```

```{r}
lsrel <- ggplot(plsr, aes(x = time_point, y = est))+
  labs(x = "Time point",y = "Reliability")+
  geom_line(aes(x = time_point), alpha = .75)+
  #geom_line(data = lsep%>%filter(time_point > 14), aes(group = task, x = time_point),position = position_dodge(width = 0.2), alpha = .75)+
  geom_pointrange(aes(ymin = lower_2.5ci, ymax = upper_2.5ci),alpha = .75, fatten = 2)+
  facet_grid(~task)+
  #scale_shape_manual(values = c(16,4), name = "", labels = c("includes 0", "different from 0"))+
  #geom_vline(xintercept = 14.5, size = 1)+
  scale_x_continuous(breaks = c(1,10), labels = c(1,10))+
  scale_y_continuous(breaks = c(0,1), limits = c(0,1))+
  theme_few()+
  theme(strip.placement = "outside", strip.text = element_blank(),
    axis.text = element_text(size = 8),       # Smaller axis text
    axis.title = element_text(size = 10),     # Smaller axis titles
    legend.text = element_text(size = 2),     # Smaller legend text
    )
```

```{r lseplot, fig.height = 4, fig.width=8, fig.cap = "Latent means and reliability estimates with 95\\% CI for each time point based on LSM. The sample size varied between time points and can be found in Supplementary Figure 1. Means at time point 1 are set to 0.", out.width="100%"}
plot_grid(lsm, lsrel, nrow = 2, labels = c("A", "B"), align = "v",axis = "l", rel_heights = c(1,1))
```

```{r}
lstf_con_rel <- bind_rows(
  bind_rows(
  lstcf$parameters$unstandardized%>%
  filter(paramHeader == "Variances", 
         param %in% c("T", "S1"))%>%
  select(param, est)%>%
  pivot_wider(names_from = param, values_from = est)%>%
  mutate(consistency = `T`/(S1 + `T`))%>%select(consistency)%>%mutate(task = "communication"),
  
  
  lstvf$parameters$unstandardized%>%
  filter(paramHeader == "Variances", 
         param %in% c("T", "S1"))%>%
  select(param, est)%>%
  pivot_wider(names_from = param, values_from = est)%>%
  mutate(consistency = `T`/(S1 + `T`))%>%select(consistency)%>%mutate(task = "visible-food"),
  
  lstrf$parameters$unstandardized%>%
  filter(paramHeader == "Variances", 
         param %in% c("T", "S1"))%>%
  select(param, est)%>%
  pivot_wider(names_from = param, values_from = est)%>%
  mutate(consistency = `T`/(S1 + `T`))%>%select(consistency)%>%mutate(task = "reasoning"),
  
  
  lstpf$parameters$unstandardized%>%
  filter(paramHeader == "Variances", 
         param %in% c("T", "S1"))%>%
  select(param, est)%>%
  pivot_wider(names_from = param, values_from = est)%>%
  mutate(consistency = `T`/(S1 + `T`))%>%select(consistency)%>%mutate(task = "pop-to-sample"),
  
  
  lstif$parameters$unstandardized%>%
  filter(paramHeader == "Variances", 
         param %in% c("T", "S1"))%>%
  select(param, est)%>%
  pivot_wider(names_from = param, values_from = est)%>%
  mutate(consistency = `T`/(S1 + `T`))%>%select(consistency)%>%mutate(task = "inhibit-searched"),
  
  lstaf$parameters$unstandardized%>%
  filter(paramHeader == "Variances", 
         param %in% c("T", "S1"))%>%
  select(param, est)%>%
  pivot_wider(names_from = param, values_from = est)%>%
  mutate(consistency = `T`/(S1 + `T`))%>%select(consistency)%>%mutate(task = "attention"),
)%>%
  mutate(param = "consistency")%>%
  dplyr::rename(value = consistency),

bind_rows(
  lstcf$parameters$r2%>%filter(param == "P11")%>%mutate(task = "communication"),
  lstvf$parameters$r2%>%filter(param == "P11")%>%mutate(task = "visible-food"),
  lstrf$parameters$r2%>%filter(param == "P11")%>%mutate(task = "reasoning"),
  lstpf$parameters$r2%>%filter(param == "P11")%>%mutate(task = "pop-to-sample"),
  lstif$parameters$r2%>%filter(param == "P11")%>%mutate(task = "inhibit-searched"),
  lstaf$parameters$r2%>%filter(param == "P11")%>%mutate(task = "attention"),
)%>%
  mutate(param = "reliability")%>%
  dplyr::rename(value = est)%>%
  select(task, value, param,lower_2.5ci, upper_2.5ci)
)

```

*[Jana: The way we compute consistency does not give a confidence interval for the LST model with fixed or varying means. Any chance we can get this? In the last paper we had this at least for the model with the fixed means]*

```{r}
lstv_con_rel <- bind_rows(
  bind_rows(
  lstcv$parameters$unstandardized%>%
  filter(paramHeader == "Variances", 
         param %in% c("T", "S1"))%>%
  select(param, est)%>%
  pivot_wider(names_from = param, values_from = est)%>%
  mutate(consistency = `T`/(S1 + `T`))%>%select(consistency)%>%mutate(task = "communication"),
  
  
  lstvv$parameters$unstandardized%>%
  filter(paramHeader == "Variances", 
         param %in% c("T", "S1"))%>%
  select(param, est)%>%
  pivot_wider(names_from = param, values_from = est)%>%
  mutate(consistency = `T`/(S1 + `T`))%>%select(consistency)%>%mutate(task = "visible-food"),
  
  lstrv$parameters$unstandardized%>%
  filter(paramHeader == "Variances", 
         param %in% c("T", "S1"))%>%
  select(param, est)%>%
  pivot_wider(names_from = param, values_from = est)%>%
  mutate(consistency = `T`/(S1 + `T`))%>%select(consistency)%>%mutate(task = "reasoning"),
  
  
  lstpv$parameters$unstandardized%>%
  filter(paramHeader == "Variances", 
         param %in% c("T", "S1"))%>%
  select(param, est)%>%
  pivot_wider(names_from = param, values_from = est)%>%
  mutate(consistency = `T`/(S1 + `T`))%>%select(consistency)%>%mutate(task = "pop-to-sample"),
  
  
  lstiv$parameters$unstandardized%>%
  filter(paramHeader == "Variances", 
         param %in% c("T", "S1"))%>%
  select(param, est)%>%
  pivot_wider(names_from = param, values_from = est)%>%
  mutate(consistency = `T`/(S1 + `T`))%>%select(consistency)%>%mutate(task = "inhibit-searched"),
  
  lstav$parameters$unstandardized%>%
  filter(paramHeader == "Variances", 
         param %in% c("T", "S1"))%>%
  select(param, est)%>%
  pivot_wider(names_from = param, values_from = est)%>%
  mutate(consistency = `T`/(S1 + `T`))%>%select(consistency)%>%mutate(task = "attention"),
)%>%
  mutate(param = "consistency")%>%
  dplyr::rename(value = consistency),

bind_rows(
  lstcv$parameters$r2%>%filter(param == "P11")%>%mutate(task = "communication"),
  lstvv$parameters$r2%>%filter(param == "P11")%>%mutate(task = "visible-food"),
  lstrv$parameters$r2%>%filter(param == "P11")%>%mutate(task = "reasoning"),
  lstpv$parameters$r2%>%filter(param == "P11")%>%mutate(task = "pop-to-sample"),
  lstiv$parameters$r2%>%filter(param == "P11")%>%mutate(task = "inhibit-searched"),
  lstav$parameters$r2%>%filter(param == "P11")%>%mutate(task = "attention"),
)%>%
  mutate(param = "reliability")%>%
  dplyr::rename(value = est)%>%
  select(task, value, param,lower_2.5ci, upper_2.5ci)
)
```


```{r}
lst_overview <- bind_rows(
lstf_con_rel%>%mutate(type = "fixed means"),
lstv_con_rel%>%mutate(type = "varying means")
)
```


```{r}
plste <- bind_rows(
lstf_con_rel%>%mutate(type = "fixed means"),
lstv_con_rel%>%mutate(type = "varying means")
)%>%
  mutate(task = recode(task,
       "pop-to-sample" = "Population-to-sample",
       "communication" = "Communicative-cues",
       "reasoning" = "Logical-reasoning",
       "visible-food" = "Ignore-visible-food",
       "inhibit-searched" = "Self-ordered-search",
       "attention" = "Attention-following"
  ))%>%
  ggplot(aes(x = param, y = value, col = type))+
  geom_pointrange(aes(ymin= lower_2.5ci, ymax = upper_2.5ci), position = position_dodge(width = 0.9))+
  theme_few()+
  ylim(0,1)+
  facet_grid(~task)+
  labs(x = "", y = "LST model estimate")+
  scale_x_discrete(labels = c("Consistency", "Reliability"), guide = guide_axis(n.dodge = 2))+
  scale_color_colorblind(name = "")+
  theme(legend.direction = "vertical", legend.position = c(0.9, 0.25), legend.box = "horizontal", legend.background = element_blank(), strip.background = element_blank(),
  strip.text.y = element_blank(), axis.title.x = element_blank(), 
    axis.text = element_text(size = 8),       # Smaller axis text
    axis.title = element_text(size = 10),     # Smaller axis titles
    legend.text = element_text(size = 8),
    legend.title = element_text(size = 8),# Smaller legend text
    strip.text = element_text(size = 8)
    )


plste2 <- bind_rows(
lstf_con_rel%>%mutate(type = "fixed means"),
lstv_con_rel%>%mutate(type = "varying means")
)%>%
  mutate(task = recode(task,
       "pop-to-sample" = "Population-to-sample",
       "communication" = "Communicative-cues",
       "reasoning" = "Logical-reasoning",
       "visible-food" = "Ignore-visible-food",
       "inhibit-searched" = "Self-ordered-search",
       "attention" = "Attention-following"
  ))%>%
  ggplot(aes(x = param, y = value, col = type))+
  geom_pointrange(aes(ymin= lower_2.5ci, ymax = upper_2.5ci), position = position_dodge(width = 0.9))+
  theme_few()+
  ylim(0,1)+
  facet_grid(~task)+
  labs(x = "", y = "LST model estimate")+
  scale_x_discrete(labels = c("Consistency", "Reliability"))+#, guide = guide_axis(n.dodge = 2))+
  scale_color_colorblind(name = "")+
  theme(legend.direction = "vertical", legend.position = c(0.9, 0.25), legend.box = "horizontal", legend.background = element_blank(), strip.background = element_blank(),
  strip.text.y = element_blank(), axis.title.x = element_blank(), 
    #axis.text = element_text(size = 8),       # Smaller axis text
    #axis.title = element_text(size = 10),     # Smaller axis titles
    legend.text = element_text(size = 8),
    #legend.title = element_text(size = 8),# Smaller legend text
    #strip.text = element_text(size = 8),
   strip.text = element_blank()# Smaller plot title
    )
```

```{r lsteplot, fig.height = 3, fig.width=8, fig.cap = "Posterior mean for model parameters (with 95\\% CrI) from LSTM for the four tasks based on data.", out.width="100%"}
plste
```

```{r}
#ggsave("../visuals/comp_fix_vary_means.png", height = 3, width = 5, scale = 1.5)
```

```{r}
cor <- bind_rows(
  lsa$parameters$stdyx.standardized%>%filter(grepl("WITH",paramHeader))%>%mutate(task = "attention"),
    lsc$parameters$stdyx.standardized%>%filter(grepl("WITH",paramHeader))%>%mutate(task = "communication"),
      lsp$parameters$stdyx.standardized%>%filter(grepl("WITH",paramHeader))%>%mutate(task = "pop-to-sample"),
      lsr$parameters$stdyx.standardized%>%filter(grepl("WITH",paramHeader))%>%mutate(task = "reasoning"),
      lsi$parameters$stdyx.standardized%>%filter(grepl("WITH",paramHeader))%>%mutate(task = "inhibit-searched"),
      lsv$parameters$stdyx.standardized%>%filter(grepl("WITH",paramHeader))%>%mutate(task = "visible-food")
)%>%
  mutate(paramHeader = str_replace(paramHeader, "\\.WITH",""),
         paramHeader = factor(as.numeric(str_replace(paramHeader, "S",""))),
          param = factor(as.numeric(str_replace(param, "S",""))))%>%
  mutate(task = recode(task,
       "pop-to-sample" = "Population-to-sample",
       "communication" = "Communicative-cues",
       "reasoning" = "Logical-reasoning",
       "visible-food" = "Ignore-visible-food",
       "inhibit-searched" = "Self-ordered-search",
       "attention" = "Attention-following"
  ))

plsc <- cor%>%
  ggplot(aes(y = paramHeader, x = param, fill = est))+
    geom_tile(color = "white")+
  labs(x = "", y = "")+
  scale_fill_gradient2(low = "#CC6677", high = "#117733", mid = "#FFE066", 
   midpoint = 0.5, limit = c(0,1), space = "Lab", 
   name="Correlation", guide = "none")+
  geom_text(aes(label = round(est, 2)), size = 2)+
  scale_y_discrete(limits=rev, breaks = c(1:10))+
  scale_x_discrete(breaks = c(1:9))+
  facet_wrap(~task, ncol = 3)+
  theme_few()+
  theme(legend.position = c(0.9,0.85), legend.direction = "vertical", legend.title = element_blank(), legend.background = element_blank())


plsc2 <- cor%>%
  ggplot(aes(y = paramHeader, x = param, fill = est))+
    geom_tile(color = "white")+
  labs(x = "", y = "")+
  scale_fill_gradientn(
    colors = c("darkgrey","darkgrey", "#CC6677", "#FFE066", "#117733"),  # Define four colors
    values = scales::rescale(c(-0.1,0, 0.01, 0.5, 1)),  # Define the position of the points along the scale
    limits = c(-0.1, 1),  # Ensure values match your data range
    space = "Lab", 
    name = "Correlation",
    breaks = c(0, 0.5, 1)
  ) +
  #geom_text(aes(label = round(est, 2)), size = 2)+
  scale_y_discrete(breaks = c(2,10))+
  scale_x_discrete(breaks = c(1,9))+
  facet_wrap(~task, nrow = 1)+
  theme_few()+
  theme(legend.position = c(0.975,0.25), legend.direction = "vertical", legend.title = element_blank(), legend.background = element_blank(), strip.text = element_blank(), legend.text = element_text(size = 8), legend.key.size = unit(0.4, "cm"))

```

```{r lsplot, fig.height=4, fig.cap = "Correlations between latent state variables based on LSM for the different tasks.", out.width="100%"}
plsc
```


```{r }
plot_grid(lsm2, plsc2, plste2, ncol = 1, labels = c("A", "B", "C"), align = "v",axis = "l", rel_heights = c(1,1))
```

```{r}
ggsave("../visuals/ls_lst.png", height = 7, width = 12, scale = 1)
```

### Attention-following

To fit the models, the response categories of 0 or 1 solved trials had to be collapsed into one category due to sparsity. Furthermore, the thresholds could not be set equal for test-half 2 at time point 1 and 9 as well as test-half 1 at time point 4 due to a different number of observed categories for the respective test halves and time point combination. Latent means can still be compared across time for the state factors based on the respective other test half.

Supplementary Figure \@ref(fig:lseplot) visualizes the latent state means and reliability estimates from the LS model. Reliability was low for earlier time points and increased towards the end of the study. Latent means also slightly increased throughout the study and were significantly different from zero (i.e. time point 1) at time point 9 and 10. Supplementary Figure \@ref(fig:lsplot) gives the correlations between the latent states for the different time points. Correlations between latent states were moderate to low for time points below 7 and substantially higher from then on. Taken together, this pattern suggests substantial re-organization of individual differences that stabilized towards the end of the study. 

In the LST model with fixed means, the consistency coefficient was estimated to be `r round(lst_overview%>%filter(task == "attention", param == "consistency", type == "fixed means")%>%pull(value),2)` which was very similar to the model with variable means (`r round(lst_overview%>%filter(task == "attention", param == "consistency", type == "varying means")%>%pull(value),2)`). This means that more than 90% of true inter-individual differences are attributable to stable (trait) differences between individuals [@Jana: das beiÃt sich mit dem LS model - hast Du eine Idee warum das so ist?]. Reliability (across time points) was estimated to be low (fixed means: `r round(lst_overview%>%filter(task == "attention", param == "reliability", type == "fixed means")%>%pull(value),2)`; variable means: fixed means: `r round(lst_overview%>%filter(task == "attention", param == "reliability", type == "varying means")%>%pull(value),2)`) (see Supplementary Figure \@ref(fig:lsteplot)). However, when only considering the data from time point 7 and later, reliability was substantially higher (`r round(lstav_short$parameters$r2%>%filter(param == "P17")%>%pull(est),2)`)

Taken together, there was considerable variation both in group- and individual-level performance -- particularly in the earlier time points. Towards the end of the study, group-level performance stabilized on a level above chance (see Supplementary Figure \@ref(fig:perfplot)) and individual-differences also became more stable. Presumably, some individuals changed the way they approached the task half-way through the study -- seeing as a competitive instead of a cooperative task -- so that systematic individual differences only emerged at later time points. 


### Communicative-cues

The lowest two categories (0 and 1 correctly solved trials per test half) were collapsed due to sparsity. Thresholds could not be set equal for test half 2 at time point 2, due to a different number of observed categories for the respective test half and time-point combination. Latent means can still be compared across time.

The latent state means in the LS model steadily increased over the course of the study (see Supplementary Figure \@ref(fig:lseplot)). Reliability fluctuated on a medium to good level (range: `r round(range(plsr$est[plsr$task == "Communicative-cues"]),2)[1]` -- `r round(range(plsr$est[plsr$task == "Communicative-cues"]),2)[2]`). The correlations between latent states for the different time points were nevertheless generally high (Supplementary Figure \@ref(fig:lsplot)). 

In the LST model with fixed means, the consistency coefficient was estimated to be `r round(lst_overview%>%filter(task == "communication", param == "consistency", type == "fixed means")%>%pull(value),2)` which was somewhat lower compared to the model with variable means (`r round(lst_overview%>%filter(task == "communication", param == "consistency", type == "varying means")%>%pull(value),2)`). This means that ~ 60% to 70% of true inter-individual differences are attributable to stable (trait) differences between individuals. Reliability (across time points) was estimated to be moderate (fixed means: `r round(lst_overview%>%filter(task == "communication", param == "reliability", type == "fixed means")%>%pull(value),2)`; variable means: fixed means: `r round(lst_overview%>%filter(task == "communication", param == "reliability", type == "varying means")%>%pull(value),2)`) (see Supplementary Figure \@ref(fig:lsteplot)).

Taken together, while there was a steady increase in group-level performance (see Supplementary Figure \@ref(fig:perfplot)), individual-differences remained fairly stable. This suggests an overall learning effect that was more or less the same across individuals.

### Ignore-visible-food

The lowest two categories (0 and 1 correctly solved trials per test half) were collapsed due to sparsity. Thresholds could not be set equal for test half 1 at time point 7 and for test half 2 at time point 8 and 10, due to a different number of observed categories for the respective test half and time-point combination. Latent means can still be compared across time.

In the LS model, the latent state means steadily increased until time point 8 and decreased again for the last two time points. Yet, the latent mean at time point 10 was still significantly different from 0 (i.e. time point 1, see Supplementary Figure \@ref(fig:lseplot)). Reliability fluctuated on a medium level (range: `r round(range(plsr$est[plsr$task == "Ignore-visible-food"]),2)[1]` -- `r round(range(plsr$est[plsr$task == "Ignore-visible-food"]),2)[2]`). The correlations between latent states for the different time points were high (Supplementary Figure \@ref(fig:lsplot)), with the exception of time point 1, which did not correlate with most of the other time points.

In the LST model with fixed means, the consistency coefficient was estimated to be `r round(lst_overview%>%filter(task == "visible-food", param == "consistency", type == "fixed means")%>%pull(value),2)` which was very similar to the model with variable means (`r round(lst_overview%>%filter(task == "visible-food", param == "consistency", type == "varying means")%>%pull(value),2)`). This means that around 60% of true inter-individual differences are attributable to stable (trait) differences between individuals. Reliability (across time points) was estimated to be moderate (fixed means: `r round(lst_overview%>%filter(task == "visible-food", param == "reliability", type == "fixed means")%>%pull(value),2)`; variable means: `r round(lst_overview%>%filter(task == "visible-food", param == "reliability", type == "varying means")%>%pull(value),2)`) (see Supplementary Figure \@ref(fig:lsteplot)).

Taken together, there was a steady increase in group-level performance (see Supplementary Figure \@ref(fig:perfplot)) over time. Except of time point 1, individual-differences were fairly stable, suggesting a a fairly consistent learning effect across individuals from time point 2 onward.

### Logical-reasoning

The lowest two (0 and 1 correctly solved trials per test half) as well as the highest two categories (5 and 6 solved trials) were collapsed due to sparsity. Thresholds could not be set equal for test half 1 at time points 7, 8 and 9, due to a different number of observed categories for the respective test half and time-point combination. Latent means can still be compared across time.

There was no change in latent state means (see Supplementary Figure \@ref(fig:lseplot)). Reliability fluctuated on a low level (range: `r round(range(plsr$est[plsr$task == "Logical-reasoning"]),2)[1]` -- `r round(range(plsr$est[plsr$task == "Logical-reasoning"]),2)[2]`). The correlations between latent states for the different time points were generally low, some even negative (Supplementary Figure \@ref(fig:lsplot)). 

In the LST model with fixed means, the consistency coefficient was estimated to be `r round(lst_overview%>%filter(task == "reasoning", param == "consistency", type == "fixed means")%>%pull(value),2)` which was very similar to the model with variable means (`r round(lst_overview%>%filter(task == "reasoning", param == "consistency", type == "varying means")%>%pull(value),2)`). This means that around 80% of true inter-individual differences are attributable to stable (trait) differences between individuals. However, the reliability (across time points) was estimated to be very low, making the consistency estimate difficult to interpret (fixed means: `r round(lst_overview%>%filter(task == "reasoning", param == "reliability", type == "fixed means")%>%pull(value),2)`; variable means: fixed means: `r round(lst_overview%>%filter(task == "visible-food", param == "reliability", type == "varying means")%>%pull(value),2)`) (see Supplementary Figure \@ref(fig:lsteplot)).

In combination with the finding that group-level performance was not reliably different from chance (see Supplementary Figure \@ref(fig:perfplot)), the low correlations between latent states and low reliability estimates suggest that this task did not measure any meaningful individual-differences in logical reasoning abilities. For this reason, we did not include this task when predicting cognitive performance by external predictors. 

### Population-to-sample

The lowest two categories (0 and 1 correctly solved trials per test half) were collapsed due to sparsity. Thresholds could not be set equal for test half 1 at time point 3 and for test half 2 at time point 8 and 10, due to a different number of observed categories for the respective test half and time-point combination. Latent means can still be compared across time.

In the LS model, the latent state means did not change over time (see Supplementary Figure \@ref(fig:lseplot)). Reliability varied on a moderate level (range: `r round(range(plsr$est[plsr$task == "Population-to-sample"]),2)[1]` -- `r round(range(plsr$est[plsr$task == "Population-to-sample"]),2)[2]`). The correlations between latent states for the different time points were generally high (Supplementary Figure \@ref(fig:lsplot)). 

In the LST model with fixed means, the consistency coefficient was estimated to be `r round(lst_overview%>%filter(task == "pop-to-sample", param == "consistency", type == "fixed means")%>%pull(value),2)` which was very similar to the model with variable means (`r round(lst_overview%>%filter(task == "pop-to-sample", param == "consistency", type == "varying means")%>%pull(value),2)`). This means that around 80% of true inter-individual differences are attributable to stable (trait) differences between individuals. Reliability (across time points) was estimated to be acceptable (fixed means: `r round(lst_overview%>%filter(task == "pop-to-sample", param == "reliability", type == "fixed means")%>%pull(value),2)`; variable means: fixed means: `r round(lst_overview%>%filter(task == "pop-to-sample", param == "reliability", type == "varying means")%>%pull(value),2)`) (see Supplementary Figure \@ref(fig:lsteplot)).

In sum, the results suggest that task-level results were robust and individual-level performance was stable over time. As noted above -- and as can be seen in Supplementary Figure \@ref(fig:perfplot) -- performance on a task level was clearly above chance.

### Self-ordered-search

This task had a different response pattern compared to the other tasks. The maximum score for each trial was 2 and not 1. Furthermore, higher scores indicate worse performance (i.e. more errors). All scores of 4 and higher were collapsed into a single category due to sparsity. Thresholds could not be set equal for test half 1 at time point 4 and for test half 2 at time point 4, 5 and 10, due to a different number of observed categories for the respective test half and time-point combination. Latent means can still be compared across time except for time point 4.

From time point 6 onwards, the latent means were significanlty lower compared to the first time point (i.e. subjects made fewer errors, see Supplementary Figure \@ref(fig:lseplot)). Reliability varied between acceptable and high levels (range: `r round(range(plsr$est[plsr$task == "Self-ordered-search"]),2)[1]` -- `r round(range(plsr$est[plsr$task == "Self-ordered-search"]),2)[2]`). The correlations between latent states for the different time points were generally high (Supplementary Figure \@ref(fig:lsplot)). 

In the LST model with fixed means, the consistency coefficient was estimated to be `r round(lst_overview%>%filter(task == "inhibit-searched", param == "consistency", type == "fixed means")%>%pull(value),2)` which was very similar to the model with variable means (`r round(lst_overview%>%filter(task == "inhibit-searched", param == "consistency", type == "varying means")%>%pull(value),2)`). This means that around 80% of true inter-individual differences are attributable to stable (trait) differences between individuals. Reliability (across time points) was estimated to be acceptable (fixed means: `r round(lst_overview%>%filter(task == "inhibit-searched", param == "reliability", type == "fixed means")%>%pull(value),2)`; variable means: fixed means: `r round(lst_overview%>%filter(task == "inhibit-searched", param == "reliability", type == "varying means")%>%pull(value),2)`) (see Supplementary Figure \@ref(fig:lsteplot)).

Taken together, the results, suggest stable individual-differences with a slight group-level decrease in error rate (see Supplementary Figure\@ref(fig:perfplot)).

### Summary

The six tasks differed substantially in what they revealed about group- and individual-level variation. What stands out is the widespread change in performance over time. For all tasks except population-to-sample and logical-reasoning we observed an improvement in performance over time. This group-level change, however, has different individual-level interpretations for the different tasks. For communicative-cues, ignore-visible-food and self-ordered-search, individual differences remained relatively stable despite the group-level change suggesting stable individual differences combined with a systematic learning effect across individuals. In contrast, for attention-following, there was little stability in individual differences at earlier time points and only towards the end emerged a more stable ordering of individuals. In combination with the low reliability at earlier time points, this suggests that at least some individuals changed their response strategy in the course of the study. The combination of low reliability, chance-level performance and low correlation of latent states for logical-reasoning suggests that this task is not suited to probe individual differences in logical reasoning abilities in great apes. It is also noteworthy that the reliability estimates are on average lower compared to a previous study testing the same individuals on different tasks [@bohn2023great]. One explanation might be the increase in performance over time. At the beginning of the study, more individuals might have chosen randomly instead of using the available information provided in the task setup and the demonstrations. By definition, random variation is not reliable. With time, more and more individuals started using the abailable inforamtion so that inter-individual differences in how good they are in using it could be detected.   

## Relations between tasks {#relations-between-tasks}

```{r}
cor_mod_raw <- readModels("../outputs/Trait_model_imp_test.out")

cor_mod <- cor_mod_raw$parameters$stdyx.standardized%>%filter(grepl("WITH",paramHeader))%>%
  mutate(paramHeader = str_replace(paramHeader, "\\.WITH",""))%>%
  filter(paramHeader != "INF1", 
         param!= "INF1")%>%
  mutate(paramHeader = recode(paramHeader,
    ATTENT = "Attention-following",
    GAZE = "Gaze-following",
    CAUSE = "Causal-inference",
    INF2 = "Inference-by-exclusion",
    DELAY = "Delay-of-gratification",
    COMM = "Communicative-cues",
    SEARCH = "Self-ordered-search",
    POP = "Population-to-sample",
    QUANT = "Quantity-discrimination",
    REASON = "Logical-reasoning",
    FOOD = "Ignore-visible-food"
  ),
  param = recode(param,
    ATTENT = "Attention-following",
    GAZE = "Gaze-following",
    CAUSE = "Causal-inference",
    INF2 = "Inference-by-exclusion",
    DELAY = "Delay-of-gratification",
    COMM = "Communicative-cues",
    SEARCH = "Self-ordered-search",
    POP = "Population-to-sample",
    QUANT = "Quantity-discrimination",
    REASON = "Logical-reasoning",
    FOOD = "Ignore-visible-food"
  ))%>%
  rowwise()%>%
  mutate(id = paste(sort(c(paramHeader, param)), collapse = "_"))%>%
  separate(id, into = c("task", "task_1"), sep = "_")%>%
  arrange(task, task_1)%>%
  mutate(sig = ifelse(pval < 0.05, "bold", "plain"))

pcor_mod <- ggplot(cor_mod, aes(y = task, x = task_1, fill = est))+
  geom_tile(color = "white")+
  labs(x = "", y = "")+
  scale_fill_gradient2(low = "#CC6677", high = "#117733", mid = "white", 
  midpoint = 0, limit = c(-1,1), space = "Lab", 
  name="Correlation")+
  #ggtitle("Model based correlations")+
  geom_text(aes(label = round(est, 2), fontface = sig), size = 3)+
  scale_x_discrete(guide = guide_axis(angle = 90))+
  #scale_y_discrete(limits=rev, breaks = c(1:10))+
  #scale_x_discrete(breaks = c(1:9))+
  theme_few()+
  theme(legend.position =  c(0.15, 0.8), legend.direction = "vertical", legend.title = element_blank(), legend.background = element_blank())

```

```{r mtmplot, fig.height=8, fig.width=8, fig.cap = "Correlations between trait estimates. Bold correlations are different from zero as judged by the 95\\% CI.", out.width="100%"}
pcor_mod
```

```{r}
ggsave("../visuals/task_level_cor.png", height = 6, width = 6, scale = 1)
```

Supplementary Figure\@ref(fig:mtmplot) shows the correlations between trait estimates for the different tasks reported in the present study as well as those reported in @bohn2023great. For the tasks reported in @bohn2023great we used the data from phase 2 because it was closer in time. Overall, most correlations were not significantly different from zero (i.e. the 95% CI did include zero). Because of this low average level of correlations, we decided not to explore models with higher-order factors and will only interpret the qualitative patterns. 

Conceptually, the tasks can be clustered in the following broader domains: *social cognition* (attention-following, gaze-following, communicative-cues), *reasoning about quantities* (quantity-discrimination, population-to-sample), *executive functions* (delay-of-gratification, self-ordered-search, ignore-visible-food) and *inferential reasoning* (logical-reasoning, causal-inference, inference-by-exclusion). As a first step, we will evaluate whether we find evidence for such a clustering in the data.

There was no significant correlation between any of the social cognition tasks. Furthermore, attention-following and gaze-following did not correlate significantly with any of the other tasks and communicative-cues correlated only with causal-inference -- a result we will discuss below. Thus, and in line with previous work [@herrmann2010structure], we found no evidence for shared cognitive processes in tasks measuring different aspects of social cognition.

The two tasks measuring reasoning about quantities did correlate significantly. Both tasks require discriminating between different quantities, directly in the case of quantity-discrimination and as part of the decision making process in the case of population-to-sample. Deciding between the samples from the two populations requires discriminating between the relative quantities within each bucket from which the samples were drawn.

Within the executive functions measures, self-ordered-search and inhibit-visible-food were significantly correlated but none of the two correlated with delay-of-gratification. The significant correlation can be explained by the need to inhibit a premature response (selecting visible food or a cup that was previously rewarded) in both tasks. It has been argued that delay-of-gratification requires self control (tolerating a longer waiting time to gain a more valuable reward) over and above behavioral inhibition [@beran2015comparative]. From this point of view, individual differences in the delay-of-gratification task might be due to differences in self control and less due to differences in inhibition.    

Finally, for the three inferential reasoning measures we found a correlation between inference-by-exclusion and causal-inference. Logical-reasoning did not correlate with either (neither did it with any other task). This is not surprising given the results reported above: the observed variation in the logical-reasoning task was largely noise and did not reflect systematic individual differences. The correlation between causal-inference and inference-by-exclusion is most likely due to the fact that both tasks involve making inferences about the location of food based on reasoning about its physical properties. 

Next we turn to the correlations across domains. Perhaps the most surprising finding is the correlation between causal-inference and communicative-cues. On a closer look, the origin might be task impurity in that there are two ways to solve the causal-inference task: first, as hypothesized, by using the rattling sound to infer the location of the food. Second, by interpreting the experimenter's shaking of the cup as a communicative cue, which is very similar to the communicative-cues task. Thus, we suspect that at a least some individuals solved the task via the second route. 

Finally, there was a cluster of significant correlations between delay-of-gratification, self-ordered-search, inference-by-exclusion, causal-inference, population-to-sample and quantity discrimination. Of the 15 possible correlations, only four were non-significant. One commonality between these tasks that might -- in part -- explain this pattern is that they all benefit from sustained attention to the task. Sustained attention facilitates the processing of the experimenter's demonstrations (population-to-sample, inference-by-exclusion, causal-inference, delay-of-gratification), ones one actions on the setup (self-ordered-search) or visually complex stimuli (quantity discrimination). Tentative support for this idea comes from the analysis of relevant predictors [see @bohn2023great and below] in which `time spent in research` was selected as a relevant predictor of performance for all of these tasks except causal-inference. This predictor reflects individual's experience with experimental studies, which often involve sustained attention to distributions of food items, actions of conspecifics and/or demonstrations by experimenters. 

## Predictability {#predictability}

In the following, we describe the Projection Prediction Inference results for each task. For each task, we differentiate between relevant and irrelevant predictors and report the projected posterior distribution for relevant predictors. 

```{r}
df_trials <- data_trial%>%
  #filter(task != "inhibit_searched")%>%
  mutate(code2 = ifelse(task == "inhibit_searched" & code > 0, 0, 1),
       code = ifelse(task == "inhibit_searched", code2, code),
        )%>%
  group_by(subject, time_point,group,test_tp, task, rank, sex, rearing, age, time_in_leipzig, observer, sick_severity,test_day, le_present, le_mean, le_max, dist_present, dist_mean, dist_max, time_outdoors)%>%
  summarise(sick_severity = mean(sick_severity), 
            p = sum(code), 
            n = length(code))%>%
  mutate(observer = ifelse(is.na(observer), "no", "yes"))
```


```{r}
# grp_size <- read_csv("../saves/srm_estimates.csv")%>%
#   group_by(species)%>%
#   summarise(n = n_distinct(subject))%>%
#   pivot_wider(names_from = species, values_from = n)
#   
# 
# df_trials <- df_trials %>%
#   # relative rank of ape within species (varies between time points)
#   group_by(group, time_point) %>%
#   mutate(
#     rel_rank = case_when(
#       group == "a_chimp" ~ percent_rank(grp_size$a_chimp:1)[rank],
#       group == "b_chimp" ~ percent_rank(grp_size$b_chimp:1)[rank],
#       group == "bonobos" ~ percent_rank(grp_size$bonobo:1)[rank],
#       group == "gorillas" ~ percent_rank(grp_size$gorilla:1)[rank],
#       group == "orangs" ~ percent_rank(grp_size$orang:1)[rank]
#     )
#   ) %>%
#   ungroup()
# 
# df_trials_final <- df_trials %>%
#   ungroup()%>%
#   mutate(across(c(sick_severity,
#                   test_tp,
#                   rel_rank,
#                   le_mean,
#                   dist_mean,
#                   time_outdoors,
#                   age,
#                   time_in_leipzig),
#                 ~scale(., center = T, scale = T))) %>%
#   mutate(across(c(
#                   subject,
#                   group,
#                   sex,
#                   test_day,
#                   le_present,
#                   dist_present,
#                   rearing,
#                   observer),
#                 as_factor)) %>%
#   mutate(observer = fct_relevel(observer, "no")) %>%
#   mutate(rearing = fct_recode(rearing, "hand" = "unknown"))%>%
#   left_join(read_csv("../saves/srm_estimates.csv")%>%
#               mutate(subject = ifelse(subject == "changa", "shanga", subject))%>%
#               select(subject, time_point, mean)%>%
#               dplyr::rename("sociality" = "mean"))%>%
#   mutate(time_point = scale(time_point, center = T, scale = T))
```


```{r}
# formula for reference models
# fm <- formula(p|trials(n) ~ sick_severity +
#                 test_tp + 
#                 test_day +
#                 rel_rank +
#                 observer +
#                 age + time_in_leipzig +
#                 sex + group +
#                 rearing +
#                 le_mean + dist_mean +
#                 time_outdoors +
#                 sociality + 
#                 time_point +
#                 (1|subject))
```

```{r}
# m_att <- brm(fm,
#              family = binomial,
#              data = df_trials_final%>%filter(task == "attention"),
#               warmup = 1000, 
#               iter = 4000, 
#               cores = 4, 
#               chains = 4,
#               seed = 2024)
# 
# m_com <- brm(fm,
#              family = binomial,
#              data = df_trials_final%>%filter(task == "communication"),
#               warmup = 1000, 
#               iter = 4000, 
#               cores = 4, 
#               chains = 4,
#               seed = 2024)
# 
# m_inh <- brm(fm,
#              family = binomial,
#              data = df_trials_final%>%filter(task == "inhibit_searched"),
#               warmup = 1000, 
#               iter = 4000, 
#               cores = 4, 
#               chains = 4,
#               seed = 2024)
# 
# m_pop <- brm(fm,
#              family = binomial,
#              data = df_trials_final%>%filter(task == "population"),
#               warmup = 1000, 
#               iter = 4000, 
#               cores = 4, 
#               chains = 4,
#               seed = 2024)
# 
# m_vis <- brm(fm,
#              family = binomial,
#              data = df_trials_final%>%filter(task == "visible_food"),
#               warmup = 1000, 
#               iter = 4000, 
#               cores = 4, 
#               chains = 4,
#               seed = 2024)

```

```{r}
# all_fixed_effects <- c("sick_severity",
#                        "test_tp", 
#                        "test_day", "time_point",
#                        "rel_rank",
#                        "observer",
#                        "age", "time_in_leipzig",
#                        "sex", "group",
#                        "rearing",
#                        "le_mean", "dist_mean",
#                        "time_outdoors",
#                        "sociality")
# # delay random intercept to last place so that it doesn't soak up all the variance
# s_terms <- c("1", all_fixed_effects,
#              paste0(paste(all_fixed_effects, collapse = " + "),
#                     " + (1 | subject)"))


```


```{r, warning = FALSE}

# doParallel::registerDoParallel(40)
# 
# cvs_att <- cv_varsel(m_att,
#                      search_terms = s_terms,
#                      validate_search = T,
#                      cv_method = "LOO", 
#                      method = "forward",
#                      parallel = TRUE,
#                      seed = 2024)
#  
# saveRDS(cvs_att,"../saves/cvs_att.rds")
# 
# doParallel::registerDoParallel(40)
# 
# cvs_com <- cv_varsel(m_com,
#                      search_terms = s_terms,
#                      validate_search = T,
#                      cv_method = "LOO", 
#                      method = "forward",
#                      parallel = TRUE,
#                      seed = 2024)
#  
# saveRDS(cvs_com,"../saves/cvs_com.rds")
# 
# doParallel::registerDoParallel(40)
# 
# 
# cvs_inh <- cv_varsel(m_inh,
#                      search_terms = s_terms,
#                      validate_search = T,
#                      cv_method = "LOO", 
#                      method = "forward",
#                      parallel = TRUE,
#                      seed = 2024)
#  
# saveRDS(cvs_inh,"../saves/cvs_inh.rds")
# 
# doParallel::registerDoParallel(40)
# 
# 
# cvs_pop <- cv_varsel(m_pop,
#                      search_terms = s_terms,
#                      validate_search = T,
#                      cv_method = "LOO", 
#                      method = "forward",
#                      parallel = TRUE,
#                      seed = 2024)
#  
# saveRDS(cvs_pop,"../saves/cvs_pop.rds")
# 
# 
# doParallel::registerDoParallel(40)
# 
# cvs_vis <- cv_varsel(m_vis,
#                      search_terms = s_terms,
#                      validate_search = T,
#                      cv_method = "LOO", 
#                      method = "forward",
#                      parallel = TRUE,
#                      seed = 2024)
#  
# saveRDS(cvs_vis,"../saves/cvs_vis.rds")

```
### Attention-following

```{r}
# cvs_att <- readRDS("../saves/cvs_att.rds")
# 
# att_varselp <- summary(cvs_att, stats = c('elpd', 'rmse'))$perf_sub%>%
#   as_tibble()%>%
#   pivot_longer(cols = c(elpd,rmse), values_to = "value", names_to = "stat")%>%
#   pivot_longer(cols = c(elpd.se,rmse.se), values_to = "se", names_to = "se_term")%>%
#   rowwise()%>%
#   filter(grepl(stat,se_term))%>%
#   select(-se_term)%>%
#   mutate(ranking_fulldata = factor(ifelse(ranking_fulldata == "(Intercept)","none",ranking_fulldata)),
#          select = ifelse(ranking_fulldata  %in% c("(1 | subject)", "group", "time_point", "time_in_leipzig"), "yes", "no"))
# 
# saveRDS(att_varselp, "../saves/cvs_att_summary.rds")


att_varselp <- readRDS("../saves/cvs_att_summary.rds")%>%
  mutate(ranking_fulldata = recode(ranking_fulldata,
    time_in_leipzig = "Time spent in research",
    rearing = "Rearing history",
    group = "Group",
    observer = "Observer present",
    observer_mod = "Observer present",
    sex = "Sex",
    rel_rank = "Rank",
    rank = "Rank",
    age = "Age",
    sick_severity = "Sickness severity",
    test_tp = "Study participation (day)",
    test_day = "Study participation (time point)",
    time_outdoors = "Time spent outdoors",
    time_point = "Time point",
    dist_mean = "Disturbance",
    le_mean = "Life event",
    sociality = "Sociality",
    a_chimp = "Chimpanzee group A",
    b_chimp = "Chimpanzee group B",
    bonobos = "Bonobo",
    orangs = "Orangutan",
    gorilla = "Gorilla",
    f = "Female", 
    m = "Male",
    hand = "Hand-reared",
    mother = "Mother-reared",
    `(1 | subject)` = "(1 | Subject)"
  ))


pattsel <- att_varselp%>%
ggplot(aes(x = size, y = value))+
  #geom_hline(data = ref_com_p1, aes(yintercept = value), lty = 3)+
  geom_line(col = "black", alpha = .5)+
  geom_pointrange(aes(ymin = value - se, ymax = value + se, col = select), size = 0.5, pch = 4)+
  facet_grid(stat~., scale = "free_y")+
  scale_color_manual(name = "relevant", values = c("black", "firebrick"))+
  labs(y = "", x = "Predictors")+
  scale_x_continuous(labels = unique(att_varselp$ranking_fulldata), breaks = unique(att_varselp$size))+
  theme_few()+
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = c(0.5,0.9), legend.direction = "horizontal", legend.background = element_blank())

```


```{r}
# att_sol_terms <- c("time_point","group", "time_in_leipzig")
# proj_att_cv <- project(cvs_att,  solution_terms = att_sol_terms)
# 
# proj_samples_att <- as.matrix(proj_att_cv)%>%
#   as_tibble()%>%
#   pivot_longer(cols =c(2:7), names_to = "pred", values_to = "value")%>%
#   filter(!pred%in%c("test_tp", "sigma", "Intercept"))%>%
#   mutate(factor = ifelse(grepl("group", pred), "group", "other predictors"),
#          pred = str_remove(pred,"b_"),
#          pred = str_remove(pred,"group"),
#          pred = str_remove(pred,"rel_"),
#          species = ifelse(factor == "group", pred,factor))
# 
# saveRDS(proj_samples_att,"../saves/proj_cvs_att.rds")

proj_samples_att <- readRDS("../saves/proj_cvs_att.rds")%>%
  filter(!grepl("Intercept",pred))%>%
  mutate(pred = recode(pred,
    time_in_leipzig = "Time spent in research",
    rearing = "Rearing history",
    group = "Group",
    observer = "Observer present",
    observer_mod = "Observer present",
    sex = "Sex",
    rel_rank = "Rank",
    rank = "Rank",
    age = "Age",
    time_point = "Time point",
    sick_severity = "Sickness severity",
    test_tp = "Study participation (day)",
    test_day = "Study participation (time point)",
    time_outdoors = "Time spent outdoors",
    dist_mean = "Disturbance",
    le_mean = "Life event",
    sociality = "Sociality",
    a_chimp = "Chimpanzee group A",
    b_chimp = "Chimpanzee group B",
    bonobos = "Bonobo",
    orangs = "Orangutan",
    gorilla = "Gorilla",
    sexf = "Sex (female)",
    rearinghand = "Rearing (hand-reared)",
    observer_modyes = "Observer present",
    `(1 | subject)` = "(1 | Subject)"
  ))%>%
  mutate(factor = recode(factor,
    group = "Group",
    sex = "Sex",
    rearing = "Rearing history"
  ))
  

pattpro <- ggplot(proj_samples_att, aes(x = value, y = pred)) +
  geom_vline(xintercept = 0, lty = 2, alpha = .75)+
  geom_density_ridges(alpha = .5, scale = .7) +
  facet_wrap(~factor, scales = "free")+
  labs(y = "", x = "Estimate")+
  theme_minimal()+
  theme(panel.border = element_rect(color = "black",fill = NA,size = 1))
```

Supplementary Figure \@ref(fig:attselp) visualizes the results. Out of the 13 predictor variables we analysed, we selected `time point`, `group`, and `time spent in research` to be relevant in addition to the random intercept term. When inspecting the projected posterior distribution for `time spent in research`, we found that less experienced apes performed better. With respect to `group`, in comparison to the reference group, the Gorillas, the Chimpanzee group B performed worse while the Bonobos performed better. The results for `time point` mirrored the results above in that they suggested a better performance at later time points.

```{r attselp, fig.height=4,fig.width=10, fig.cap = "Predictor selection for attention-following. A) Elpd and RMSE values (with standard error) for predictors, ordered by importance (left to right) according to the cross-validated projection prediction model. Note that the random intercept term was forced to be the last one to be included. B) Projections for the selected predictors based on the submodel. Reference level for group are bonobos.", out.width="100%"}
ggarrange(pattsel, pattpro, labels = c("A","B"), widths = c(1,1))
```

### Communicative-cues

```{r}
# cvs_com <- readRDS("../saves/cvs_com.rds")
# 
# com_varselp <- summary(cvs_com, stats = c('elpd', 'rmse'))$perf_sub%>%
#   as_tibble()%>%
#   pivot_longer(cols = c(elpd,rmse), values_to = "value", names_to = "stat")%>%
#   pivot_longer(cols = c(elpd.se,rmse.se), values_to = "se", names_to = "se_term")%>%
#   rowwise()%>%
#   filter(grepl(stat,se_term))%>%
#   select(-se_term)%>%
#   mutate(ranking_fulldata = factor(ifelse(ranking_fulldata == "(Intercept)","none",ranking_fulldata)),
#          select = ifelse(ranking_fulldata  %in% c("(1 | subject)", "group", "time_point"), "yes", "no"))
# 
# saveRDS(com_varselp, "../saves/cvs_com_summary.rds")

com_varselp <- readRDS("../saves/cvs_com_summary.rds")%>%
  mutate(ranking_fulldata = recode(ranking_fulldata,
    time_in_leipzig = "Time spent in research",
    rearing = "Rearing history",
    group = "Group",
    observer = "Observer present",
    observer_mod = "Observer present",
    sex = "Sex",
    rel_rank = "Rank",
    rank = "Rank",
    age = "Age",
    sick_severity = "Sickness severity",
    test_tp = "Study participation (day)",
    test_day = "Study participation (time point)",
    time_outdoors = "Time spent outdoors",
    time_point = "Time point",
    dist_mean = "Disturbance",
    le_mean = "Life event",
    sociality = "Sociality",
    a_chimp = "Chimpanzee group A",
    b_chimp = "Chimpanzee group B",
    bonobos = "Bonobo",
    orangs = "Orangutan",
    gorilla = "Gorilla",
    f = "Female", 
    m = "Male",
    hand = "Hand-reared",
    mother = "Mother-reared",
    `(1 | subject)` = "(1 | Subject)"
  ))


pcomsel <- com_varselp%>%
ggplot(aes(x = size, y = value))+
  #geom_hline(data = ref_com_p1, aes(yintercept = value), lty = 3)+
  geom_line(col = "black", alpha = .5)+
  geom_pointrange(aes(ymin = value - se, ymax = value + se, col = select), size = 0.5, pch = 4)+
  facet_grid(stat~., scale = "free_y")+
  scale_color_manual(name = "relevant", values = c("black", "firebrick"))+
  labs(y = "", x = "Predictors")+
  scale_x_continuous(labels = unique(com_varselp$ranking_fulldata), breaks = unique(com_varselp$size))+
  theme_few()+
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = c(0.5,0.9), legend.direction = "horizontal", legend.background = element_blank())


```


```{r}
# com_sol_terms <- c("time_point","group")
# proj_com_cv <- project(cvs_com,  solution_terms = com_sol_terms)
# 
# proj_samples_com <- as.matrix(proj_com_cv)%>%
#   as_tibble()%>%
#   pivot_longer(cols =c(2:6), names_to = "pred", values_to = "value")%>%
#   filter(!pred%in%c("test_tp", "sigma", "Intercept"))%>%
#   mutate(factor = ifelse(grepl("group", pred), "group", "other predictors"),
#          pred = str_remove(pred,"b_"),
#          pred = str_remove(pred,"group"),
#          pred = str_remove(pred,"rel_"),
#          species = ifelse(factor == "group", pred,factor))
# 
# saveRDS(proj_samples_com,"../saves/proj_cvs_com.rds")

proj_samples_com <- readRDS("../saves/proj_cvs_com.rds")%>%
  filter(!grepl("Intercept",pred))%>%
  mutate(pred = recode(pred,
    time_in_leipzig = "Time spent in research",
    rearing = "Rearing history",
    group = "Group",
    observer = "Observer present",
    observer_mod = "Observer present",
    sex = "Sex",
    rel_rank = "Rank",
    rank = "Rank",
    age = "Age",
    time_point = "Time point",
    sick_severity = "Sickness severity",
    test_tp = "Study participation (day)",
    test_day = "Study participation (time point)",
    time_outdoors = "Time spent outdoors",
    dist_mean = "Disturbance",
    le_mean = "Life event",
    sociality = "Sociality",
    a_chimp = "Chimpanzee group A",
    b_chimp = "Chimpanzee group B",
    bonobos = "Bonobo",
    orangs = "Orangutan",
    gorilla = "Gorilla",
    sexf = "Sex (female)",
    rearinghand = "Rearing (hand-reared)",
    observer_modyes = "Observer present",
    `(1 | subject)` = "(1 | Subject)"
  ))%>%
  mutate(factor = recode(factor,
    group = "Group",
    sex = "Sex",
    rearing = "Rearing history"
  ))
  

pcompro <- ggplot(proj_samples_com, aes(x = value, y = pred)) +
  geom_vline(xintercept = 0, lty = 2, alpha = .75)+
  geom_density_ridges(alpha = .5, scale = .7) +
  facet_wrap(~factor, scales = "free")+
  labs(y = "", x = "Estimate")+
  theme_minimal()+
  theme(panel.border = element_rect(color = "black",fill = NA,size = 1))
```


Supplementary Figure \@ref(fig:comselp) visualizes the results. The predictors for the communicative-cues task were `time point` and `group`. The results for `time point` mirrored the results above in that they suggested a better performance at later time points. With respect to `group`, in comparison to the reference group, the Gorillas, all other groups performed better, in particular the Chimpanzee group B.

```{r comselp, fig.height=4,fig.width=10, fig.cap = "Predictor selection for attention-following. A) Elpd and RMSE values (with standard error) for predictors, ordered by importance (left to right) according to the cross-validated projection prediction model. Note that the random intercept term was forced to be the last one to be included. B) Projections for the selected predictors based on the submodel. Reference level for group are bonobos.", out.width="100%"}
ggarrange(pcomsel, pcompro, labels = c("A","B"), widths = c(1,1))
```

### Ignore-visible-food


```{r}
# cvs_vis <- readRDS("../saves/cvs_vis.rds")
# 
# vis_varselp <- summary(cvs_vis, stats = c('elpd', 'rmse'))$perf_sub%>%
#   as_tibble()%>%
#   pivot_longer(cols = c(elpd,rmse), values_to = "value", names_to = "stat")%>%
#   pivot_longer(cols = c(elpd.se,rmse.se), values_to = "se", names_to = "se_term")%>%
#   rowwise()%>%
#   filter(grepl(stat,se_term))%>%
#   select(-se_term)%>%
#   mutate(ranking_fulldata = factor(ifelse(ranking_fulldata == "(Intercept)","none",ranking_fulldata)),
#          select = ifelse(ranking_fulldata  %in% c("(1 | subject)", "time_point", "group", "sex", "time_in_leipzig", "rearing"), "yes", "no"))
# 
# saveRDS(vis_varselp, "../saves/cvs_vis_summary.rds")

vis_varselp <- readRDS("../saves/cvs_vis_summary.rds")%>%
  mutate(ranking_fulldata = recode(ranking_fulldata,
                                   time_in_leipzig = "Time spent in research",
                                   rearing = "Rearing history",
                                   group = "Group",
                                   observer = "Observer present",
                                   observer_mod = "Observer present",
                                   sex = "Sex",
                                   rel_rank = "Rank",
                                   rank = "Rank",
                                   age = "Age",
                                   sick_severity = "Sickness severity",
                                   test_tp = "Study participation (day)",
                                   test_day = "Study participation (time point)",
                                   time_outdoors = "Time spent outdoors",
                                   time_point = "Time point",
                                   dist_mean = "Disturbance",
                                   le_mean = "Life event",
                                   sociality = "Sociality",
                                   a_chimp = "Chimpanzee group A",
                                   b_chimp = "Chimpanzee group B",
                                   bonobos = "Bonobo",
                                   orangs = "Orangutan",
                                   gorilla = "Gorilla",
                                   f = "Female", 
                                   m = "Male",
                                   hand = "Hand-reared",
                                   mother = "Mother-reared",
                                   `(1 | subject)` = "(1 | Subject)"
  ))


pvissel <- vis_varselp%>%
  ggplot(aes(x = size, y = value))+
  #geom_hline(data = ref_com_p1, aes(yintercept = value), lty = 3)+
  geom_line(col = "black", alpha = .5)+
  geom_pointrange(aes(ymin = value - se, ymax = value + se, col = select), size = 0.5, pch = 4)+
  facet_grid(stat~., scale = "free_y")+
  scale_color_manual(name = "relevant", values = c("black", "firebrick"))+
  labs(y = "", x = "Predictors")+
  scale_x_continuous(labels = unique(vis_varselp$ranking_fulldata), breaks = unique(vis_varselp$size))+
  theme_few()+
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = c(0.5,0.6), legend.direction = "horizontal", legend.background = element_blank())

```


```{r}
# vis_sol_terms <- c("time_point", "group", "sex", "time_in_leipzig", "rearing")
# proj_vis_cv <- project(cvs_vis,  solution_terms = vis_sol_terms)
# 
# proj_samples_vis <- as.matrix(proj_vis_cv)%>%
#   as_tibble()%>%
#   pivot_longer(cols =c(2:9), names_to = "pred", values_to = "value")%>%
#   filter(!pred%in%c("test_tp", "sigma", "Intercept"))%>%
#   mutate(factor = ifelse(grepl("group", pred), "group", "other predictors"),
#          pred = str_remove(pred,"b_"),
#          pred = str_remove(pred,"group"),
#          pred = str_remove(pred,"rel_"),
#          species = ifelse(factor == "group", pred,factor))
# 
# saveRDS(proj_samples_vis,"../saves/proj_cvs_vis.rds")

proj_samples_vis <- readRDS("../saves/proj_cvs_vis.rds")%>%
  filter(!grepl("Intercept",pred))%>%
  mutate(pred = recode(pred,
                       time_in_leipzig = "Time spent in research",
                       rearing = "Rearing history",
                       group = "Group",
                       observer = "Observer present",
                       observer_mod = "Observer present",
                       sex = "Sex",
                       rel_rank = "Rank",
                       rank = "Rank",
                       age = "Age",
                       time_point = "Time point",
                       sick_severity = "Sickness severity",
                       test_tp = "Study participation (day)",
                       test_day = "Study participation (time point)",
                       time_outdoors = "Time spent outdoors",
                       dist_mean = "Disturbance",
                       le_mean = "Life event",
                       sociality = "Sociality",
                       a_chimp = "Chimpanzee group A",
                       b_chimp = "Chimpanzee group B",
                       bonobos = "Bonobo",
                       orangs = "Orangutan",
                       gorilla = "Gorilla",
                       sexf = "Sex (female)",
                       rearinghand = "Rearing (hand-reared)",
                       observer_modyes = "Observer present",
                       `(1 | subject)` = "(1 | Subject)"
  ))%>%
  mutate(factor = recode(factor,
                         group = "Group",
                         sex = "Sex",
                         rearing = "Rearing history"
  ))


pvispro <- ggplot(proj_samples_vis, aes(x = value, y = pred)) +
  geom_vline(xintercept = 0, lty = 2, alpha = .75)+
  geom_density_ridges(alpha = .5, scale = .7) +
  facet_wrap(~factor, scales = "free")+
  labs(y = "", x = "Estimate")+
  theme_minimal()+
  theme(panel.border = element_rect(color = "black",fill = NA,size = 1))
```

Supplementary Figure \@ref(fig:visselp) visualizes the results. The selected predictors were `time point`, `group`, `sex`, `time spent in research` and `rearing history`. Once again, the results for `time point` mirrored the results above in that they suggested a better performance at later time points. For `group`, in comparison to the reference group, the Gorillas, all other groups performed worse. The results for `time spent in research` suggest that more experience apes performed better. Finally, female (`sex`) and hand-reared (`rearing history`) individuals performed worse. 

```{r visselp, fig.height=4,fig.width=10, fig.cap = "Predictor selection for ignore-visible-food. A) Elpd and RMSE values (with standard error) for predictors, ordered by importance (left to right) according to the cross-validated projection prediction model. Note that the random intercept term was forced to be the last one to be included. B) Projections for the selected predictors based on the submodel. Reference level for group are bonobos.", out.width="100%"}
ggarrange(pvissel, pvispro, labels = c("A","B"), widths = c(1,1))
```

### Population-to-sample

```{r}
# cvs_pop <- readRDS("../saves/cvs_pop.rds")
# 
# pop_varselp <- summary(cvs_pop, stats = c('elpd', 'rmse'))$perf_sub%>%
#   as_tibble()%>%
#   pivot_longer(cols = c(elpd,rmse), values_to = "value", names_to = "stat")%>%
#   pivot_longer(cols = c(elpd.se,rmse.se), values_to = "se", names_to = "se_term")%>%
#   rowwise()%>%
#   filter(grepl(stat,se_term))%>%
#   select(-se_term)%>%
#   mutate(ranking_fulldata = factor(ifelse(ranking_fulldata == "(Intercept)","none",ranking_fulldata)),
#          select = ifelse(ranking_fulldata  %in% c("(1 | subject)", "time_in_leipzig", "group", "rearing", "sex", "rel_rank"), "yes", "no"))
# 
# saveRDS(pop_varselp, "../saves/cvs_pop_summary.rds")

pop_varselp <- readRDS("../saves/cvs_pop_summary.rds")%>%
  mutate(ranking_fulldata = recode(ranking_fulldata,
                                   time_in_leipzig = "Time spent in research",
                                   rearing = "Rearing history",
                                   group = "Group",
                                   observer = "Observer present",
                                   observer_mod = "Observer present",
                                   sex = "Sex",
                                   rel_rank = "Rank",
                                   rank = "Rank",
                                   age = "Age",
                                   sick_severity = "Sickness severity",
                                   test_tp = "Study participation (day)",
                                   test_day = "Study participation (time point)",
                                   time_outdoors = "Time spent outdoors",
                                   time_point = "Time point",
                                   dist_mean = "Disturbance",
                                   le_mean = "Life event",
                                   sociality = "Sociality",
                                   a_chimp = "Chimpanzee group A",
                                   b_chimp = "Chimpanzee group B",
                                   bonobos = "Bonobo",
                                   orangs = "Orangutan",
                                   gorilla = "Gorilla",
                                   f = "Female", 
                                   m = "Male",
                                   hand = "Hand-reared",
                                   mother = "Mother-reared",
                                   `(1 | subject)` = "(1 | Subject)"
  ))


ppopsel <- pop_varselp%>%
  ggplot(aes(x = size, y = value))+
  #geom_hline(data = ref_com_p1, aes(yintercept = value), lty = 3)+
  geom_line(col = "black", alpha = .5)+
  geom_pointrange(aes(ymin = value - se, ymax = value + se, col = select), size = 0.5, pch = 4)+
  facet_grid(stat~., scale = "free_y")+
  scale_color_manual(name = "relevant", values = c("black", "firebrick"))+
  labs(y = "", x = "Predictors")+
  scale_x_continuous(labels = unique(pop_varselp$ranking_fulldata), breaks = unique(pop_varselp$size))+
  theme_few()+
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = c(0.5,0.9), legend.direction = "horizontal", legend.background = element_blank())

```


```{r}
# pop_sol_terms <- c("time_in_leipzig", "group", "rearing", "sex", "rel_rank")
# proj_pop_cv <- project(cvs_pop,  solution_terms = pop_sol_terms)
# 
# proj_samples_pop <- as.matrix(proj_pop_cv)%>%
#   as_tibble()%>%
#   pivot_longer(cols =c(2:9), names_to = "pred", values_to = "value")%>%
#   filter(!pred%in%c("test_tp", "sigma", "Intercept"))%>%
#   mutate(factor = ifelse(grepl("group", pred), "group", "other predictors"),
#          pred = str_remove(pred,"b_"),
#          pred = str_remove(pred,"group"),
#          pred = str_remove(pred,"rel_"),
#          species = ifelse(factor == "group", pred,factor))
# 
# saveRDS(proj_samples_pop,"../saves/proj_cvs_pop.rds")

proj_samples_pop <- readRDS("../saves/proj_cvs_pop.rds")%>%
  filter(!grepl("Intercept",pred))%>%
  mutate(pred = recode(pred,
                       time_in_leipzig = "Time spent in research",
                       rearing = "Rearing history",
                       group = "Group",
                       observer = "Observer present",
                       observer_mod = "Observer present",
                       sex = "Sex",
                       rel_rank = "Rank",
                       rank = "Rank",
                       age = "Age",
                       time_point = "Time point",
                       sick_severity = "Sickness severity",
                       test_tp = "Study participation (day)",
                       test_day = "Study participation (time point)",
                       time_outdoors = "Time spent outdoors",
                       dist_mean = "Disturbance",
                       le_mean = "Life event",
                       sociality = "Sociality",
                       a_chimp = "Chimpanzee group A",
                       b_chimp = "Chimpanzee group B",
                       bonobos = "Bonobo",
                       orangs = "Orangutan",
                       gorilla = "Gorilla",
                       sexf = "Sex (female)",
                       rearinghand = "Rearing (hand-reared)",
                       observer_modyes = "Observer present",
                       `(1 | subject)` = "(1 | Subject)"
  ))%>%
  mutate(factor = recode(factor,
                         group = "Group",
                         sex = "Sex",
                         rearing = "Rearing history"
  ))


ppoppro <- ggplot(proj_samples_pop, aes(x = value, y = pred)) +
  geom_vline(xintercept = 0, lty = 2, alpha = .75)+
  geom_density_ridges(alpha = .5, scale = .7) +
  facet_wrap(~factor, scales = "free")+
  labs(y = "", x = "Estimate")+
  theme_minimal()+
  theme(panel.border = element_rect(color = "black",fill = NA,size = 1))
```

Supplementary Figure \@ref(fig:popselp) visualizes the results. The selected predictors were `time spent in research`, `rearing history`,  `group`, `sex` and `rank`. The results for `time spent in research` suggest that more experience apes performed better. All groups except the Chimpnazee A grup outperformed the Gorillas. Females (`sex`) performed better while hand-reared (`rearing histpry`) individuals performed worse. Finally, higher ranking (`rank`) individuals performed better. 

```{r popselp, fig.height=4,fig.width=10, fig.cap = "Predictor selection for population-to-sample. A) Elpd and RMSE values (with standard error) for predictors, ordered by importance (left to right) according to the cross-validated projection prediction model. Note that the random intercept term was forced to be the last one to be included. B) Projections for the selected predictors based on the submodel. Reference level for group are bonobos.", out.width="100%"}
ggarrange(ppopsel, ppoppro, labels = c("A","B"), widths = c(1,1))
```


### Self-ordered-search

```{r}
# cvs_inh <- readRDS("../saves/cvs_inh.rds")
# 
# inh_varselp <- summary(cvs_inh, stats = c('elpd', 'rmse'))$perf_sub%>%
#   as_tibble()%>%
#   pivot_longer(cols = c(elpd,rmse), values_to = "value", names_to = "stat")%>%
#   pivot_longer(cols = c(elpd.se,rmse.se), values_to = "se", names_to = "se_term")%>%
#   rowwise()%>%
#   filter(grepl(stat,se_term))%>%
#   select(-se_term)%>%
#   mutate(ranking_fulldata = factor(ifelse(ranking_fulldata == "(Intercept)","none",ranking_fulldata)),
#          select = ifelse(ranking_fulldata  %in% c("(1 | subject)", "group", "time_point", "time_in_leipzig", "rearing", "observer", "age", "sex" ), "yes", "no"))
# 
# saveRDS(inh_varselp, "../saves/cvs_inh_summary.rds")

inh_varselp <- readRDS("../saves/cvs_inh_summary.rds")%>%
  mutate(ranking_fulldata = recode(ranking_fulldata,
    time_in_leipzig = "Time spent in research",
    rearing = "Rearing history",
    group = "Group",
    observer = "Observer present",
    observer_mod = "Observer present",
    sex = "Sex",
    rel_rank = "Rank",
    rank = "Rank",
    age = "Age",
    sick_severity = "Sickness severity",
    test_tp = "Study participation (day)",
    test_day = "Study participation (time point)",
    time_outdoors = "Time spent outdoors",
    time_point = "Time point",
    dist_mean = "Disturbance",
    le_mean = "Life event",
    sociality = "Sociality",
    a_chimp = "Chimpanzee group A",
    b_chimp = "Chimpanzee group B",
    bonobos = "Bonobo",
    orangs = "Orangutan",
    gorilla = "Gorilla",
    f = "Female", 
    m = "Male",
    hand = "Hand-reared",
    mother = "Mother-reared",
    `(1 | subject)` = "(1 | Subject)"
  ))


pinhsel <- inh_varselp%>%
ggplot(aes(x = size, y = value))+
  #geom_hline(data = ref_com_p1, aes(yintercept = value), lty = 3)+
  geom_line(col = "black", alpha = .5)+
  geom_pointrange(aes(ymin = value - se, ymax = value + se, col = select), size = 0.5, pch = 4)+
  facet_grid(stat~., scale = "free_y")+
  scale_color_manual(name = "relevant", values = c("black", "firebrick"))+
  labs(y = "", x = "Predictors")+
  scale_x_continuous(labels = unique(inh_varselp$ranking_fulldata), breaks = unique(inh_varselp$size))+
  theme_few()+
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = c(0.5,0.9), legend.direction = "horizontal", legend.background = element_blank())

```


```{r}
# inh_sol_terms <- c("group", "time_point", "time_in_leipzig", "rearing", "observer", "age", "sex")
# proj_inh_cv <- project(cvs_inh,  solution_terms = inh_sol_terms)
# 
# proj_samples_inh <- as.matrix(proj_inh_cv)%>%
#   as_tibble()%>%
#   pivot_longer(cols =c(2:11), names_to = "pred", values_to = "value")%>%
#   filter(!pred%in%c("test_tp", "sigma", "Intercept"))%>%
#   mutate(factor = ifelse(grepl("group", pred), "group", "other predictors"),
#          pred = str_remove(pred,"b_"),
#          pred = str_remove(pred,"group"),
#          pred = str_remove(pred,"rel_"),
#          species = ifelse(factor == "group", pred,factor))
# 
# saveRDS(proj_samples_inh,"../saves/proj_cvs_inh.rds")

proj_samples_inh <- readRDS("../saves/proj_cvs_inh.rds")%>%
  filter(!grepl("Intercept",pred))%>%
  mutate(pred = recode(pred,
    time_in_leipzig = "Time spent in research",
    rearing = "Rearing history",
    group = "Group",
    observer = "Observer present",
    observer_mod = "Observer present",
    sex = "Sex",
    rel_rank = "Rank",
    rank = "Rank",
    age = "Age",
    time_point = "Time point",
    sick_severity = "Sickness severity",
    test_tp = "Study participation (day)",
    test_day = "Study participation (time point)",
    time_outdoors = "Time spent outdoors",
    dist_mean = "Disturbance",
    le_mean = "Life event",
    sociality = "Sociality",
    a_chimp = "Chimpanzee group A",
    b_chimp = "Chimpanzee group B",
    bonobos = "Bonobo",
    orangs = "Orangutan",
    gorilla = "Gorilla",
    sexf = "Sex (female)",
    rearinghand = "Rearing (hand-reared)",
    observeryes = "Observer present",
    `(1 | subject)` = "(1 | Subject)"
  ))%>%
  mutate(factor = recode(factor,
    group = "Group",
    sex = "Sex",
    rearing = "Rearing history"
  ))
  

pinhpro <- ggplot(proj_samples_inh, aes(x = value, y = pred)) +
  geom_vline(xintercept = 0, lty = 2, alpha = .75)+
  geom_density_ridges(alpha = .5, scale = .7) +
  facet_wrap(~factor, scales = "free")+
  labs(y = "", x = "Estimate")+
  theme_minimal()+
  theme(panel.border = element_rect(color = "black",fill = NA,size = 1))
```

Please note that the dependent variable for this task was transformed to match the structure of the other tasks. As described above, we coded trials with no redundant search as "correct" and trials with one or more redundant searches as "incorrect". 

Supplementary Figure \@ref(fig:inhselp) visualizes the results. The selected predictors were `time point`, `time spent in research`,  `group`, `rearing history`, `Observer present`, `age`, and `sex`. Once again, performance increased over time (`time point`). More experience apes performed better (`time spent in research`). In comparison to the Gorillas, all other groups performed worse, with the Chimpanzee group B being closest. Hand-reared (`rearing histpry`) individuals performed worse. Performance was worse when an observer was present. Performance was better for older individuals. Finally, males (`sex`) performed better. 

```{r inhselp, fig.height=4,fig.width=10, fig.cap = "Predictor selection for self-ordeered-search. A) Elpd and RMSE values (with standard error) for predictors, ordered by importance (left to right) according to the cross-validated projection prediction model. Note that the random intercept term was forced to be the last one to be included. B) Projections for the selected predictors based on the submodel. Reference level for group are bonobos.", out.width="100%"}
ggarrange(pinhsel, pinhpro, labels = c("A","B"), widths = c(1,1))
```

### Summary

```{r}
ro <- bind_rows(
  com_varselp%>%mutate(task = "Communicative-cues"),
  vis_varselp%>%mutate(task = "Ignore-visible-food"),
  att_varselp%>%mutate(task = "Attention-following"),
  inh_varselp%>%mutate(task = "Self-ordered-search"),
  pop_varselp%>%mutate(task = "Population-to-sample")

)%>%
  filter(stat == "elpd", 
         ranking_fulldata != "none",
         ranking_fulldata != "day2",
         ranking_fulldata != "(1|subject)",
         ranking_fulldata != "(1 | Subject)",
         ranking_fulldata != "(time_point | subject)",
         ranking_fulldata != "(time point | Subject)")%>%
  select(size, ranking_fulldata, task, select)%>%
  group_by(ranking_fulldata)%>%
  mutate(sum = sum(size))%>%
  ungroup()%>%
  droplevels()%>%
  mutate(type = ifelse(
    ranking_fulldata == "Time spent in research" | ranking_fulldata == "Rearing history" | ranking_fulldata == "Group" | ranking_fulldata == "Sex"| ranking_fulldata == "Age", "Stable charac.", ifelse(
      ranking_fulldata == "Rank" | ranking_fulldata == "Sickness severity" | ranking_fulldata == "Sociality" ,"Variable charac.", ifelse(
        ranking_fulldata == "Time spent outdoors" | ranking_fulldata == "Disturbance" | ranking_fulldata == "Life event", "Group life", "Testing arr."
      )
    )
  ))%>%
  mutate(ranking_fulldata = recode(ranking_fulldata, 
                                   "Study participation (time point)" = "Study part. (time point)",
                                   "Study participation (day)" = "Study part. (day)",
                                   "Time spent in research" = "Time in research"))


ppicomp <- ggplot(ro,aes(x = fct_reorder(ranking_fulldata,sum), y = size, col = type ))+
  geom_line(aes(group = task), alpha = .5, col = "black")+
  geom_point(aes(pch = select), size = 2, stroke = 1)+
  theme_minimal()+
  facet_wrap(~task, ncol = 1)+
  labs(x = "", y = "Rank based on PPI model")+
  scale_color_brewer(palette="Set1", name = "")+
  scale_linetype(name = "")+
  scale_shape_manual(name = "", values =  c(1,4), labels = c("Irrelevant", "Relevant"))+
  scale_y_reverse(breaks = c(15,10,5,1), limits = c(16,0))+
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.box = "vertical",
        legend.direction = "vertical",
        legend.position = "right", 
        legend.spacing.y = unit(0, 'cm'),
        #legend.spacing.x = unit(0, 'cm'),
        panel.border  = element_rect(colour = "black",fill = NA)
        )

```

```{r}
#ggsave("../visuals/proj_sel.png", height = 4, width = 12, scale = 1, bg = "white")
```

```{r ppisum, fig.height=8, fig.width=8, fig.cap = "Predictor ranking and selection based on PPI models. Crosses mark predictors that were selected to be relevant based on the PPI models. Color shows the broader category each predictor belongs to. The x-axis is sorted by the average rank across tasks.", out.width="100%"}
ppicomp
```


```{r}
proj_samples_sum <- bind_rows(
  proj_samples_com%>%mutate(task = "Communicative-cues"),
  proj_samples_vis%>%mutate(task = "Ignore-visible-food"),
  proj_samples_att%>%mutate(task = "Attention-following"),
  proj_samples_inh%>%mutate(task = "Self-ordered-search"),
  proj_samples_pop%>%mutate(task = "Population-to-sample")
)%>%
  group_by(task,factor, pred)%>%
  summarise(mean = mean(value),
            lci = hdi_lower(value),
            uci = hdi_upper(value))%>%
  mutate(factor = ifelse(factor == "other predictors", "Other predictors", factor))


ppipreds <- ggplot(proj_samples_sum, aes(x = mean, y = pred, col = task))+
  geom_vline(xintercept = 0, lty = 3, alpha = 0.75)+
  geom_pointrange(aes(xmin = lci, xmax = uci), position = position_dodge(width = -0.3))+
  facet_wrap(factor~., scales = "free_y", nrow = 2)+
  scale_color_colorblind()+
  labs(x = "", y = "")+
  theme_minimal()+
  theme(#axis.text.x = element_text(angle = 45, hjust = 1),
        #legend.box = "vertical",
        #legend.direction = "vertical",
        legend.position = "right", 
        #legend.spacing.y = unit(0, 'cm'),
        #legend.spacing.x = unit(0, 'cm'),
        panel.border  = element_rect(colour = "black",fill = NA)
        )
```
```{r}
#ggsave("../visuals/pred_rank.png", height = 8, width = 10, scale = 1, bg = "white")
```

```{r ppipreds, fig.height=6, fig.width=10, fig.cap = "Posterior model estimates for the selected predictors for each task based on data. Points show means with 95\\% Credible Interval. Color denotes task. For categorical predictors, the estimate gives the difference compared to the reference level (Gorilla for group).", out.width="100%"}
ppipreds
```

```{r}
plot_grid(ppicomp, ppipreds, nrow = 1, labels = c("A", "B"), rel_widths = c(1,1))
```
```{r}
ggsave("../visuals/pred_sum.png", height = 6, width = 12, scale = 1, bg = "white")
```

Supplementary Figure \@ref(fig:ppisum) summarizes the selected predictors across tasks. Supplementary Figure \@ref(fig:ppipreds) shows the projected posterior model estimates for the predictors that were selected as relevant. 

Across tasks, the random intercept term `(1 | subject)` was the predictor that improved model fit the most. In line with results reported by @bohn2023great, this suggests that idiosyncratic developmental processes or genetic pre-dispositions, which operate on a much longer time-scale than what we captured in our study, account for a substantial portion of the variance in cognitive abilities between individuals. 

However, for two tasks, other predictors had an comparable explanatory power -- something we did not observe in @bohn2023great. For population-to-sample, `time spent in research` improved the model fit even more than adding the random intercept at the end did. This could be interpreted that performance in this task strongly depends on having learned to pay attention to stimuli and the human experimenter. For ignore-visible-food, `time point` had an influence exceeding that of the random intercept term. We think this result reflects the strong within-task learning effect across subjects. Because performance increased substantially with time, most of the variation captured by `time point` exceeded the variation between individuals.  

For the remaining predictors, the most highly-ranked and frequently selected ones came from the group of stable individual characteristics. The big exception being `time point`, which was ranked second across tasks. This pattern aligns with the SEM results, in which we saw that most of the variance in performance could be traced back to stable trait differences between individuals. The remaining occasion specific variation was largely due to continuous improvement over time, most likely reflecting task-specific learning processes. The remaining time-varying predictors did not account for much variation over and above stable trait differnences and learning.  

The predictor selected most often was `group`. It ws the only predictor that was selected as relevant for all tasks. Differences between groups were, however, variable in that the ranking of the groups  changed from task to task. For example, the Gorillas performed best in ignore-visible-food and self-ordered-search, the Chimpanzee group B performed best in communicative-cues and population-to-sample and the Bonobos performed best in attention-following. This speaks against clear species or group differences in general cognitive performance. Again, the most likely explanation for group differences is an interaction between species specific dispositions and individual- / task-level developmental processes.

The predictors that were selected more than once influenced performance in variable ways. As mentioned above, `time point` always had a positive effect because performance increased with time. Whenever `rearing` was selected to be relevant, mother-reared individuals outperformed others. `Time spent in research` had a positive effect, suggesting that more experience with research leads to better performance, except for attention-following. The effect of `sex` was variable in that females outperformed males in population-to-sample but males outperformed females in self-ordered-search and ignore-visible-food. 

# Supplementary References

::: {#refs}
:::
